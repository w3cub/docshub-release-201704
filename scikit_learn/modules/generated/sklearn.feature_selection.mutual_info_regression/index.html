
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>sklearn.feature_selection.mutual_info_regression() - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content=" Estimate mutual information for a continuous target variable. ">
  <meta name="keywords" content="sklearn, feature, selection, mutual, info, regression, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/generated/sklearn.feature_selection.mutual_info_regression/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="sklearn-feature-selection-mutual-info-regression">sklearn.feature_selection.mutual_info_regression</h1> <dl class="function"> <dt id="sklearn.feature_selection.mutual_info_regression">
<code>sklearn.feature_selection.mutual_info_regression(X, y, discrete_features='auto', n_neighbors=3, copy=True, random_state=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/feature_selection/mutual_info_.py#L290" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Estimate mutual information for a continuous target variable.</p> <p>Mutual information (MI) <a class="reference internal" href="#r173" id="id1">[R173]</a> between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.</p> <p>The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in <a class="reference internal" href="#r174" id="id2">[R174]</a> and <a class="reference internal" href="#r175" id="id3">[R175]</a>. Both methods are based on the idea originally proposed in <a class="reference internal" href="#r176" id="id4">[R176]</a>.</p> <p>It can be used for univariate features selection, read more in the <a class="reference internal" href="../../feature_selection/#univariate-feature-selection"><span class="std std-ref">User Guide</span></a>.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : array_like or sparse matrix, shape (n_samples, n_features)</p>  <p>Feature matrix.</p>  <p><strong>y</strong> : array_like, shape (n_samples,)</p>  <p>Target vector.</p>  <p><strong>discrete_features</strong> : {‘auto’, bool, array_like}, default ‘auto’</p>  <p>If bool, then determines whether to consider all features discrete or continuous. If array, then it should be either a boolean mask with shape (n_features,) or array with indices of discrete features. If ‘auto’, it is assigned to False for dense <code>X</code> and to True for sparse <code>X</code>.</p>  <p><strong>n_neighbors</strong> : int, default 3</p>  <p>Number of neighbors to use for MI estimation for continuous variables, see <a class="reference internal" href="#r174" id="id5">[R174]</a> and <a class="reference internal" href="#r175" id="id6">[R175]</a>. Higher values reduce variance of the estimation, but could introduce a bias.</p>  <p><strong>copy</strong> : bool, default True</p>  <p>Whether to make a copy of the given data. If set to False, the initial data will be overwritten.</p>  <p><strong>random_state</strong> : int seed, RandomState instance or None, default None</p>  <p>The seed of the pseudo random number generator for adding small noise to continuous variables in order to remove repeated values.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>mi</strong> : ndarray, shape (n_features,)</p>  <p>Estimated mutual information between each feature and the target.</p>  </td> </tr>  </table> <h4 class="rubric">Notes</h4> <ol class="arabic simple"> <li>The term “discrete features” is used instead of naming them “categorical”, because it describes the essence more accurately. For example, pixel intensities of an image are discrete features (but hardly categorical) and you will get better results if mark them as such. Also note, that treating a continuous variable as discrete and vice versa will usually give incorrect results, so be attentive about that.</li> <li>True mutual information can’t be negative. If its estimate turns out to be negative, it is replaced by zero.</li> </ol> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="r173" rules="none">   <tr>
<td class="label">[R173]</td>
<td>
<em>(<a class="fn-backref" href="#id1">1</a>, <a class="fn-backref" href="#id7">2</a>)</em> <a class="reference external" href="https://en.wikipedia.org/wiki/Mutual_information" target="_blank">Mutual Information</a> on Wikipedia.</td>
</tr>  </table> <table class="docutils citation" frame="void" id="r174" rules="none">   <tr>
<td class="label">[R174]</td>
<td>
<em>(<a class="fn-backref" href="#id2">1</a>, <a class="fn-backref" href="#id5">2</a>, <a class="fn-backref" href="#id8">3</a>)</em> A. Kraskov, H. Stogbauer and P. Grassberger, “Estimating mutual information”. Phys. Rev. E 69, 2004.</td>
</tr>  </table> <table class="docutils citation" frame="void" id="r175" rules="none">   <tr>
<td class="label">[R175]</td>
<td>
<em>(<a class="fn-backref" href="#id3">1</a>, <a class="fn-backref" href="#id6">2</a>, <a class="fn-backref" href="#id9">3</a>)</em> B. C. Ross “Mutual Information between Discrete and Continuous Data Sets”. PLoS ONE 9(2), 2014.</td>
</tr>  </table> <table class="docutils citation" frame="void" id="r176" rules="none">   <tr>
<td class="label">[R176]</td>
<td>
<em>(<a class="fn-backref" href="#id4">1</a>, <a class="fn-backref" href="#id10">2</a>)</em> L. F. Kozachenko, N. N. Leonenko, “Sample Estimate of the Entropy of a Random Vector”, Probl. Peredachi Inf., 23:2 (1987), 9-16</td>
</tr>  </table> </dd>
</dl>  <h2 id="examples-using-sklearn-feature-selection-mutual-info-regression">Examples using <code>sklearn.feature_selection.mutual_info_regression</code>
</h2> <div class="sphx-glr-thumbcontainer" tooltip="This example illustrates the differences between univariate F-test statistics and mutual inform...">
<div class="figure" id="id11"> <img alt="../../_images/sphx_glr_plot_f_test_vs_mi_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_f_test_vs_mi_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/feature_selection/plot_f_test_vs_mi/#sphx-glr-auto-examples-feature-selection-plot-f-test-vs-mi-py"><span class="std std-ref">Comparison of F-test and mutual information</span></a></span></p> </div> </div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
