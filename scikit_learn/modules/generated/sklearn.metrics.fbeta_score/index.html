
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>sklearn.metrics.fbeta_score() - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content=" Compute the F-beta score ">
  <meta name="keywords" content="sklearn, metrics, fbeta, score, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/generated/sklearn.metrics.fbeta_score/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="sklearn-metrics-fbeta-score">sklearn.metrics.fbeta_score</h1> <dl class="function"> <dt id="sklearn.metrics.fbeta_score">
<code>sklearn.metrics.fbeta_score(y_true, y_pred, beta, labels=None, pos_label=1, average='binary', sample_weight=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/metrics/classification.py#L695" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Compute the F-beta score</p> <p>The F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0.</p> <p>The <code>beta</code> parameter determines the weight of precision in the combined score. <code>beta &lt; 1</code> lends more weight to precision, while <code>beta &gt; 1</code> favors recall (<code>beta -&gt; 0</code> considers only precision, <code>beta -&gt; inf</code> only recall).</p> <p>Read more in the <a class="reference internal" href="../../model_evaluation/#precision-recall-f-measure-metrics"><span class="std std-ref">User Guide</span></a>.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>y_true</strong> : 1d array-like, or label indicator array / sparse matrix</p>  <p>Ground truth (correct) target values.</p>  <p><strong>y_pred</strong> : 1d array-like, or label indicator array / sparse matrix</p>  <p>Estimated targets as returned by a classifier.</p>  <p><strong>beta: float</strong> :</p>  <p>Weight of precision in harmonic mean.</p>  <p><strong>labels</strong> : list, optional</p>  <p>The set of labels to include when <code>average != 'binary'</code>, and their order if <code>average is None</code>. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in <code>y_true</code> and <code>y_pred</code> are used in sorted order.</p> <div class="versionchanged"> <p><span class="versionmodified">Changed in version 0.17: </span>parameter <em>labels</em> improved for multiclass problem.</p> </div>  <p><strong>pos_label</strong> : str or int, 1 by default</p>  <p>The class to report if <code>average='binary'</code> and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting <code>labels=[pos_label]</code> and <code>average != 'binary'</code> will report scores for that label only.</p>  <p><strong>average</strong> : string, [None, ‘binary’ (default), ‘micro’, ‘macro’, ‘samples’, ‘weighted’]</p>  <p>This parameter is required for multiclass/multilabel targets. If <code>None</code>, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:</p> <dl class="docutils"> <dt>
<code>'binary':</code> </dt> <dd>
<p class="first last">Only report results for the class specified by <code>pos_label</code>. This is applicable only if targets (<code>y_{true,pred}</code>) are binary.</p> </dd> <dt>
<code>'micro':</code> </dt> <dd>
<p class="first last">Calculate metrics globally by counting the total true positives, false negatives and false positives.</p> </dd> <dt>
<code>'macro':</code> </dt> <dd>
<p class="first last">Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.</p> </dd> <dt>
<code>'weighted':</code> </dt> <dd>
<p class="first last">Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.</p> </dd> <dt>
<code>'samples':</code> </dt> <dd>
<p class="first last">Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from <a class="reference internal" href="../sklearn.metrics.accuracy_score/#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code>accuracy_score</code></a>).</p> </dd> </dl>  <p><strong>sample_weight</strong> : array-like of shape = [n_samples], optional</p>  <p>Sample weights.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>fbeta_score</strong> : float (if average is not None) or array of float, shape = [n_unique_labels]</p>  <p>F-beta score of the positive class in binary classification or weighted average of the F-beta score of each class for the multiclass task.</p>  </td> </tr>  </table> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="r206" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id1">[R206]</a></td>
<td>R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern Information Retrieval. Addison Wesley, pp. 327-328.</td>
</tr>  </table> <table class="docutils citation" frame="void" id="r207" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id2">[R207]</a></td>
<td><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score" target="_blank">Wikipedia entry for the F1-score</a></td>
</tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from sklearn.metrics import fbeta_score
&gt;&gt;&gt; y_true = [0, 1, 2, 0, 1, 2]
&gt;&gt;&gt; y_pred = [0, 2, 1, 0, 0, 1]
&gt;&gt;&gt; fbeta_score(y_true, y_pred, average='macro', beta=0.5)
... 
0.23...
&gt;&gt;&gt; fbeta_score(y_true, y_pred, average='micro', beta=0.5)
... 
0.33...
&gt;&gt;&gt; fbeta_score(y_true, y_pred, average='weighted', beta=0.5)
... 
0.23...
&gt;&gt;&gt; fbeta_score(y_true, y_pred, average=None, beta=0.5)
... 
array([ 0.71...,  0.        ,  0.        ])
</pre> </dd>
</dl> <div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
