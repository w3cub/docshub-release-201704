
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>2.9. Neural Network Models - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content="Restricted Boltzmann machines (RBM) are unsupervised nonlinear feature learners based on a probabilistic model. The features extracted by an RBM or &hellip;">
  <meta name="keywords" content="neural, network, models, unsupervised, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/neural_networks_unsupervised/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="neural-networks-unsupervised">2.9. Neural network models (unsupervised)</h1>  <h2 id="rbm">2.9.1. Restricted Boltzmann machines</h2> <p id="neural-network-models-unsupervised">Restricted Boltzmann machines (RBM) are unsupervised nonlinear feature learners based on a probabilistic model. The features extracted by an RBM or a hierarchy of RBMs often give good results when fed into a linear classifier such as a linear SVM or a perceptron.</p> <p>The model makes assumptions regarding the distribution of inputs. At the moment, scikit-learn only provides <a class="reference internal" href="../generated/sklearn.neural_network.bernoullirbm/#sklearn.neural_network.BernoulliRBM" title="sklearn.neural_network.BernoulliRBM"><code>BernoulliRBM</code></a>, which assumes the inputs are either binary values or values between 0 and 1, each encoding the probability that the specific feature would be turned on.</p> <p>The RBM tries to maximize the likelihood of the data using a particular graphical model. The parameter learning algorithm used (<a class="reference internal" href="#sml"><span class="std std-ref">Stochastic Maximum Likelihood</span></a>) prevents the representations from straying far from the input data, which makes them capture interesting regularities, but makes the model less useful for small datasets, and usually not useful for density estimation.</p> <p>The method gained popularity for initializing deep neural networks with the weights of independent RBMs. This method is known as unsupervised pre-training.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/neural_networks/plot_rbm_logistic_classification/"><img alt="../_images/sphx_glr_plot_rbm_logistic_classification_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_rbm_logistic_classification_0011.png" style="width: 420.0px; height: 400.0px;"></a> </div> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li><a class="reference internal" href="../../auto_examples/neural_networks/plot_rbm_logistic_classification/#sphx-glr-auto-examples-neural-networks-plot-rbm-logistic-classification-py"><span class="std std-ref">Restricted Boltzmann Machine features for digit classification</span></a></li> </ul> </div>  <h3 id="graphical-model-and-parametrization">2.9.1.1. Graphical model and parametrization</h3> <p>The graphical model of an RBM is a fully-connected bipartite graph.</p> <img alt="../_images/rbm_graph.png" class="align-center" src="http://scikit-learn.org/stable/_images/rbm_graph.png"> <p>The nodes are random variables whose states depend on the state of the other nodes they are connected to. The model is therefore parameterized by the weights of the connections, as well as one intercept (bias) term for each visible and hidden unit, ommited from the image for simplicity.</p> <p>The energy function measures the quality of a joint assignment:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/df25bb39991ae6423d7b0cda4bae7f5475b6240e.png" alt="E(\mathbf{v}, \mathbf{h}) = \sum_i \sum_j w_{ij}v_ih_j + \sum_i b_iv_i
  + \sum_j c_jh_j"></p> </div>
<p>In the formula above, <img class="math" src="http://scikit-learn.org/stable/_images/math/c426364a800b8b3ac0709d5582365e0711316e4c.png" alt="\mathbf{b}"> and <img class="math" src="http://scikit-learn.org/stable/_images/math/86c5269f5d1ecdd2c651815e55e2d19febcb5ad4.png" alt="\mathbf{c}"> are the intercept vectors for the visible and hidden layers, respectively. The joint probability of the model is defined in terms of the energy:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/383caadc7cec723d9573d789e639ad9304e086d3.png" alt="P(\mathbf{v}, \mathbf{h}) = \frac{e^{-E(\mathbf{v}, \mathbf{h})}}{Z}"></p> </div>
<p>The word <em>restricted</em> refers to the bipartite structure of the model, which prohibits direct interaction between hidden units, or between visible units. This means that the following conditional independencies are assumed:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/84e4e92a838c9c2606e614e21a07db667dffe7dd.png" alt="h_i \bot h_j | \mathbf{v} \\
v_i \bot v_j | \mathbf{h}"></p> </div>
<p>The bipartite structure allows for the use of efficient block Gibbs sampling for inference.</p>   <h3 id="bernoulli-restricted-boltzmann-machines">2.9.1.2. Bernoulli Restricted Boltzmann machines</h3> <p>In the <a class="reference internal" href="../generated/sklearn.neural_network.bernoullirbm/#sklearn.neural_network.BernoulliRBM" title="sklearn.neural_network.BernoulliRBM"><code>BernoulliRBM</code></a>, all units are binary stochastic units. This means that the input data should either be binary, or real-valued between 0 and 1 signifying the probability that the visible unit would turn on or off. This is a good model for character recognition, where the interest is on which pixels are active and which aren’t. For images of natural scenes it no longer fits because of background, depth and the tendency of neighbouring pixels to take the same values.</p> <p>The conditional probability distribution of each unit is given by the logistic sigmoid activation function of the input it receives:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/0ae6a567590ed7a63b1e86cab2dc037d3e09861b.png" alt="P(v_i=1|\mathbf{h}) = \sigma(\sum_j w_{ij}h_j + b_i) \\
P(h_i=1|\mathbf{v}) = \sigma(\sum_i w_{ij}v_i + c_j)"></p> </div>
<p>where <img class="math" src="http://scikit-learn.org/stable/_images/math/011e5790a6c33043ceadca81d9657dde6c61d769.png" alt="\sigma"> is the logistic sigmoid function:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/09fc64e9724b4c1ee2ec4c4798456f9b01eff845.png" alt="\sigma(x) = \frac{1}{1 + e^{-x}}"></p> </div>  <h3 id="sml">2.9.1.3. Stochastic Maximum Likelihood learning</h3> <p id="stochastic-maximum-likelihood-learning">The training algorithm implemented in <a class="reference internal" href="../generated/sklearn.neural_network.bernoullirbm/#sklearn.neural_network.BernoulliRBM" title="sklearn.neural_network.BernoulliRBM"><code>BernoulliRBM</code></a> is known as Stochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence (PCD). Optimizing maximum likelihood directly is infeasible because of the form of the data likelihood:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/9a1fa677e1f48439082b7d6c27bb597047c37665.png" alt="\log P(v) = \log \sum_h e^{-E(v, h)} - \log \sum_{x, y} e^{-E(x, y)}"></p> </div>
<p>For simplicity the equation above is written for a single training example. The gradient with respect to the weights is formed of two terms corresponding to the ones above. They are usually known as the positive gradient and the negative gradient, because of their respective signs. In this implementation, the gradients are estimated over mini-batches of samples.</p> <p>In maximizing the log-likelihood, the positive gradient makes the model prefer hidden states that are compatible with the observed training data. Because of the bipartite structure of RBMs, it can be computed efficiently. The negative gradient, however, is intractable. Its goal is to lower the energy of joint states that the model prefers, therefore making it stay true to the data. It can be approximated by Markov chain Monte Carlo using block Gibbs sampling by iteratively sampling each of <img class="math" src="http://scikit-learn.org/stable/_images/math/8d230554a01423c4c6560104b2918b65607c9406.png" alt="v"> and <img class="math" src="http://scikit-learn.org/stable/_images/math/293fb39e1b93282c804a86186e721b32f829f1b2.png" alt="h"> given the other, until the chain mixes. Samples generated in this way are sometimes refered as fantasy particles. This is inefficient and it is difficult to determine whether the Markov chain mixes.</p> <p>The Contrastive Divergence method suggests to stop the chain after a small number of iterations, <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k">, usually even 1. This method is fast and has low variance, but the samples are far from the model distribution.</p> <p>Persistent Contrastive Divergence addresses this. Instead of starting a new chain each time the gradient is needed, and performing only one Gibbs sampling step, in PCD we keep a number of chains (fantasy particles) that are updated <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"> Gibbs steps after each weight update. This allows the particles to explore the space more thoroughly.</p> <div class="topic"> <p class="topic-title first">References:</p> <ul class="simple"> <li>
<a class="reference external" href="http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf" target="_blank">“A fast learning algorithm for deep belief nets”</a> G. Hinton, S. Osindero, Y.-W. Teh, 2006</li> <li>
<a class="reference external" href="http://www.cs.toronto.edu/~tijmen/pcd/pcd.pdf" target="_blank">“Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient”</a> T. Tieleman, 2008</li> </ul> </div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/neural_networks_unsupervised.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/neural_networks_unsupervised.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
