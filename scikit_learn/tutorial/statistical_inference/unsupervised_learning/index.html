
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Tutorial&#58; Unsupervised Learning - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content=" The problem solved in clustering ">
  <meta name="keywords" content="unsupervised, learning, seeking, representations, data, tutorial, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/tutorial/statistical_inference/unsupervised_learning/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="unsupervised-learning-seeking-representations-of-the-data">Unsupervised learning: seeking representations of the data</h1>  <h2 id="clustering-grouping-observations-together">Clustering: grouping observations together</h2> <div class="topic"> <p class="topic-title first">The problem solved in clustering</p> <p>Given the iris dataset, if we knew that there were 3 types of iris, but did not have access to a taxonomist to label them: we could try a <strong>clustering task</strong>: split the observations into well-separated group called <em>clusters</em>.</p> </div>  <h3 id="k-means-clustering">K-means clustering</h3> <p>Note that there exist a lot of different clustering criteria and associated algorithms. The simplest clustering algorithm is <a class="reference internal" href="../../../modules/clustering/#k-means"><span class="std std-ref">K-means</span></a>.</p> <a class="reference external image-reference" href="../../../auto_examples/cluster/plot_cluster_iris/"><img alt="../../_images/sphx_glr_plot_cluster_iris_002.png" class="align-right" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_iris_002.png" style="width: 280.0px; height: 210.0px;"></a> <pre data-language="python">&gt;&gt;&gt; from sklearn import cluster, datasets
&gt;&gt;&gt; iris = datasets.load_iris()
&gt;&gt;&gt; X_iris = iris.data
&gt;&gt;&gt; y_iris = iris.target

&gt;&gt;&gt; k_means = cluster.KMeans(n_clusters=3)
&gt;&gt;&gt; k_means.fit(X_iris) 
KMeans(algorithm='auto', copy_x=True, init='k-means++', ...
&gt;&gt;&gt; print(k_means.labels_[::10])
[1 1 1 1 1 0 0 0 0 0 2 2 2 2 2]
&gt;&gt;&gt; print(y_iris[::10])
[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]
</pre> <div class="admonition warning"> <p class="first admonition-title">Warning</p> <p>There is absolutely no guarantee of recovering a ground truth. First, choosing the right number of clusters is hard. Second, the algorithm is sensitive to initialization, and can fall into local minima, although scikit-learn employs several tricks to mitigate this issue.</p> <table class="centered docutils">   <tr class="row-odd">
<td><a class="reference external" href="../../../auto_examples/cluster/plot_cluster_iris/"><img alt="k_means_iris_bad_init" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_iris_003.png" style="width: 252.0px; height: 189.0px;"></a></td> <td><a class="reference external" href="../../../auto_examples/cluster/plot_cluster_iris/"><img alt="k_means_iris_8" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_iris_001.png" style="width: 252.0px; height: 189.0px;"></a></td> <td><a class="reference external" href="../../../auto_examples/cluster/plot_cluster_iris/"><img alt="cluster_iris_truth" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_iris_004.png" style="width: 252.0px; height: 189.0px;"></a></td> </tr> <tr class="row-even">
<td><strong>Bad initialization</strong></td> <td><strong>8 clusters</strong></td> <td><strong>Ground truth</strong></td> </tr>  </table> <p class="last"><strong>Don’t over-interpret clustering results</strong></p> </div> <div class="topic"> <p class="topic-title first"><strong>Application example: vector quantization</strong></p> <p>Clustering in general and KMeans, in particular, can be seen as a way of choosing a small number of exemplars to compress the information. The problem is sometimes known as <a class="reference external" href="https://en.wikipedia.org/wiki/Vector_quantization" target="_blank">vector quantization</a>. For instance, this can be used to posterize an image:</p> <pre data-language="python">&gt;&gt;&gt; import scipy as sp
&gt;&gt;&gt; try:
...    face = sp.face(gray=True)
... except AttributeError:
...    from scipy import misc
...    face = misc.face(gray=True)
&gt;&gt;&gt; X = face.reshape((-1, 1)) # We need an (n_sample, n_feature) array
&gt;&gt;&gt; k_means = cluster.KMeans(n_clusters=5, n_init=1)
&gt;&gt;&gt; k_means.fit(X) 
KMeans(algorithm='auto', copy_x=True, init='k-means++', ...
&gt;&gt;&gt; values = k_means.cluster_centers_.squeeze()
&gt;&gt;&gt; labels = k_means.labels_
&gt;&gt;&gt; face_compressed = np.choose(labels, values)
&gt;&gt;&gt; face_compressed.shape = face.shape
</pre> <table class="centered docutils">   <tr class="row-odd">
<td><a class="reference external" href="../../../auto_examples/cluster/plot_face_compress/"><img alt="face" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_face_compress_001.png" style="width: 180.0px; height: 132.0px;"></a></td> <td><a class="reference external" href="../../../auto_examples/cluster/plot_face_compress/"><img alt="face_compressed" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_face_compress_003.png" style="width: 180.0px; height: 132.0px;"></a></td> <td><a class="reference external" href="../../../auto_examples/cluster/plot_face_compress/"><img alt="face_regular" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_face_compress_002.png" style="width: 180.0px; height: 132.0px;"></a></td> <td><a class="reference external" href="../../../auto_examples/cluster/plot_face_compress/"><img alt="face_histogram" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_face_compress_004.png" style="width: 180.0px; height: 132.0px;"></a></td> </tr> <tr class="row-even">
<td>Raw image</td> <td>K-means quantization</td> <td>Equal bins</td> <td>Image histogram</td> </tr>  </table> </div>   <h3 id="hierarchical-agglomerative-clustering-ward">Hierarchical agglomerative clustering: Ward</h3> <p>A <a class="reference internal" href="../../../modules/clustering/#hierarchical-clustering"><span class="std std-ref">Hierarchical clustering</span></a> method is a type of cluster analysis that aims to build a hierarchy of clusters. In general, the various approaches of this technique are either:</p>  <ul class="simple"> <li>
<strong>Agglomerative</strong> - bottom-up approaches: each observation starts in its own cluster, and clusters are iterativelly merged in such a way to minimize a <em>linkage</em> criterion. This approach is particularly interesting when the clusters of interest are made of only a few observations. When the number of clusters is large, it is much more computationally efficient than k-means.</li> <li>
<strong>Divisive</strong> - top-down approaches: all observations start in one cluster, which is iteratively split as one moves down the hierarchy. For estimating large numbers of clusters, this approach is both slow (due to all observations starting as one cluster, which it splits recursively) and statistically ill-posed.</li> </ul>   <h4 id="connectivity-constrained-clustering">Connectivity-constrained clustering</h4> <p>With agglomerative clustering, it is possible to specify which samples can be clustered together by giving a connectivity graph. Graphs in the scikit are represented by their adjacency matrix. Often, a sparse matrix is used. This can be useful, for instance, to retrieve connected regions (sometimes also referred to as connected components) when clustering an image:</p> <a class="reference external image-reference" href="../../../auto_examples/cluster/plot_face_ward_segmentation/"><img alt="../../_images/sphx_glr_plot_face_ward_segmentation_001.png" class="align-right" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_face_ward_segmentation_001.png" style="width: 200.0px; height: 200.0px;"></a> <pre data-language="python">
import matplotlib.pyplot as plt

from sklearn.feature_extraction.image import grid_to_graph
from sklearn.cluster import AgglomerativeClustering
from sklearn.utils.testing import SkipTest
from sklearn.utils.fixes import sp_version

if sp_version &lt; (0, 12):
    raise SkipTest("Skipping because SciPy version earlier than 0.12.0 and "
                   "thus does not include the scipy.misc.face() image.")


###############################################################################
# Generate data
try:
    face = sp.face(gray=True)
except AttributeError:
    # Newer versions of scipy have face in misc
    from scipy import misc
    face = misc.face(gray=True)

# Resize it to 10% of the original size to speed up the processing
face = sp.misc.imresize(face, 0.10) / 255.

</pre>   <h4 id="feature-agglomeration">Feature agglomeration</h4> <p>We have seen that sparsity could be used to mitigate the curse of dimensionality, <em>i.e</em> an insufficient amount of observations compared to the number of features. Another approach is to merge together similar features: <strong>feature agglomeration</strong>. This approach can be implemented by clustering in the feature direction, in other words clustering the transposed data.</p> <a class="reference external image-reference" href="../../../auto_examples/cluster/plot_digits_agglomeration/"><img alt="../../_images/sphx_glr_plot_digits_agglomeration_001.png" class="align-right" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_digits_agglomeration_001.png" style="width: 228.0px; height: 199.5px;"></a> <pre data-language="python">&gt;&gt;&gt; digits = datasets.load_digits()
&gt;&gt;&gt; images = digits.images
&gt;&gt;&gt; X = np.reshape(images, (len(images), -1))
&gt;&gt;&gt; connectivity = grid_to_graph(*images[0].shape)

&gt;&gt;&gt; agglo = cluster.FeatureAgglomeration(connectivity=connectivity,
...                                      n_clusters=32)
&gt;&gt;&gt; agglo.fit(X) 
FeatureAgglomeration(affinity='euclidean', compute_full_tree='auto',...
&gt;&gt;&gt; X_reduced = agglo.transform(X)

&gt;&gt;&gt; X_approx = agglo.inverse_transform(X_reduced)
&gt;&gt;&gt; images_approx = np.reshape(X_approx, images.shape)
</pre> <div class="topic"> <p class="topic-title first"><code>transform</code> and <code>inverse_transform</code> methods</p> <p>Some estimators expose a <code>transform</code> method, for instance to reduce the dimensionality of the dataset.</p> </div>     <h2 id="decompositions-from-a-signal-to-components-and-loadings">Decompositions: from a signal to components and loadings</h2> <div class="topic"> <p class="topic-title first"><strong>Components and loadings</strong></p> <p>If X is our multivariate data, then the problem that we are trying to solve is to rewrite it on a different observational basis: we want to learn loadings L and a set of components C such that <em>X = L C</em>. Different criteria exist to choose the components</p> </div>  <h3 id="principal-component-analysis-pca">Principal component analysis: PCA</h3> <p><a class="reference internal" href="../../../modules/decomposition/#pca"><span class="std std-ref">Principal component analysis (PCA)</span></a> selects the successive components that explain the maximum variance in the signal.</p> <p class="centered"><a class="reference external" href="../../../auto_examples/decomposition/plot_pca_3d/"><img alt="pca_3d_axis" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_pca_3d_001.png" style="width: 280.0px; height: 210.0px;"></a> <a class="reference external" href="../../../auto_examples/decomposition/plot_pca_3d/"><img alt="pca_3d_aligned" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_pca_3d_002.png" style="width: 280.0px; height: 210.0px;"></a></p> <p>The point cloud spanned by the observations above is very flat in one direction: one of the three univariate features can almost be exactly computed using the other two. PCA finds the directions in which the data is not <em>flat</em></p> <p>When used to <em>transform</em> data, PCA can reduce the dimensionality of the data by projecting on a principal subspace.</p> <pre data-language="python">&gt;&gt;&gt; # Create a signal with only 2 useful dimensions
&gt;&gt;&gt; x1 = np.random.normal(size=100)
&gt;&gt;&gt; x2 = np.random.normal(size=100)
&gt;&gt;&gt; x3 = x1 + x2
&gt;&gt;&gt; X = np.c_[x1, x2, x3]

&gt;&gt;&gt; from sklearn import decomposition
&gt;&gt;&gt; pca = decomposition.PCA()
&gt;&gt;&gt; pca.fit(X)
PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)
&gt;&gt;&gt; print(pca.explained_variance_)  
[  2.18565811e+00   1.19346747e+00   8.43026679e-32]

&gt;&gt;&gt; # As we can see, only the 2 first components are useful
&gt;&gt;&gt; pca.n_components = 2
&gt;&gt;&gt; X_reduced = pca.fit_transform(X)
&gt;&gt;&gt; X_reduced.shape
(100, 2)
</pre>   <h3 id="independent-component-analysis-ica">Independent Component Analysis: ICA</h3> <p><a class="reference internal" href="../../../modules/decomposition/#ica"><span class="std std-ref">Independent component analysis (ICA)</span></a> selects components so that the distribution of their loadings carries a maximum amount of independent information. It is able to recover <strong>non-Gaussian</strong> independent signals:</p> <a class="reference external image-reference" href="../../../auto_examples/decomposition/plot_ica_blind_source_separation/"><img alt="../../_images/sphx_glr_plot_ica_blind_source_separation_001.png" class="align-center" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_ica_blind_source_separation_001.png" style="width: 560.0px; height: 420.0px;"></a> <pre data-language="python">&gt;&gt;&gt; # Generate sample data
&gt;&gt;&gt; time = np.linspace(0, 10, 2000)
&gt;&gt;&gt; s1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal
&gt;&gt;&gt; s2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal
&gt;&gt;&gt; S = np.c_[s1, s2]
&gt;&gt;&gt; S += 0.2 * np.random.normal(size=S.shape)  # Add noise
&gt;&gt;&gt; S /= S.std(axis=0)  # Standardize data
&gt;&gt;&gt; # Mix data
&gt;&gt;&gt; A = np.array([[1, 1], [0.5, 2]])  # Mixing matrix
&gt;&gt;&gt; X = np.dot(S, A.T)  # Generate observations

&gt;&gt;&gt; # Compute ICA
&gt;&gt;&gt; ica = decomposition.FastICA()
&gt;&gt;&gt; S_ = ica.fit_transform(X)  # Get the estimated sources
&gt;&gt;&gt; A_ = ica.mixing_.T
&gt;&gt;&gt; np.allclose(X,  np.dot(S_, A_) + ica.mean_)
True
</pre>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
