
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>AudioContext - DOM - W3cubDocs</title>
  
  <meta name="description" content="The AudioContext interface represents an audio-processing graph built from audio modules linked together, each represented by an AudioNode. An audio &hellip;">
  <meta name="keywords" content="audiocontext, -, dom">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/dom/audiocontext/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/dom.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/dom/" class="_nav-link" title="" style="margin-left:0;">DOM</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _mdn">
				
<h1>AudioContext</h1> <div> <p>The <code>AudioContext</code> interface represents an audio-processing graph built from audio modules linked together, each represented by an <a href="../audionode/" title="The AudioNode interface is a generic interface for representing an audio processing module like an audio source (e.g. an HTML &lt;audio&gt; or &lt;video&gt; element, an OscillatorNode, etc.), the audio destination, intermediate processing module (e.g. a filter like BiquadFilterNode or ConvolverNode), or volume control (like GainNode)."><code>AudioNode</code></a>. An audio context controls both the creation of the nodes it contains and the execution of the audio processing, or decoding. You need to create an AudioContext before you do anything else, as everything happens inside a context.</p> </div> <p>An <code>AudioContext</code> can be a target of events, therefore it implements the <a href="../eventtarget/" title="EventTarget is an interface implemented by objects that can receive events and may have listeners for them."><code>EventTarget</code></a> interface.</p> <h2 id="Constructor">Constructor</h2> <dl> <dt><a href="../audiocontext/audiocontext/" title="The AudioContext() constructor creates a new AudioContext object which represents an audio-processing graph built from audio modules linked together, each represented by an AudioNode."><code>AudioContext()</code></a></dt> <dd>Creates and returns a new <code>AudioContext</code> object.</dd> </dl> <h2 id="Properties">Properties</h2> <dl> <dt>
<a href="../audiocontext/currenttime/" title="The currentTime read-only property of the AudioContext interface returns a double representing an ever-increasing hardware timestamp in seconds that can be used for scheduling audio playback, visualizing timelines, etc. It starts at 0."><code>AudioContext.currentTime</code></a> <span class="inlineIndicator readOnly readOnlyInline" title="This value may not be changed.">Read only </span>
</dt> <dd>Returns a double representing an ever-increasing hardware time in seconds used for scheduling. It starts at <code>0</code>.</dd> <dt>
<a href="../audiocontext/destination/" title="An AudioDestinationNode."><code>AudioContext.destination</code></a> <span class="inlineIndicator readOnly readOnlyInline" title="This value may not be changed.">Read only </span>
</dt> <dd>Returns an <a href="../audiodestinationnode/" title="AudioDestinationNode has no output (as it is the output, no more AudioNode can be linked after it in the audio graph) and one input. The amount of channels in the input must be between 0 and the maxChannelCount value or an exception is raised."><code>AudioDestinationNode</code></a> representing the final destination of all audio in the context. It can be thought of as the audio-rendering device.</dd> <dt>
<a href="../audiocontext/listener/" title="An AudioListener object."><code>AudioContext.listener</code></a> <span class="inlineIndicator readOnly readOnlyInline" title="This value may not be changed.">Read only </span>
</dt> <dd>Returns the <a href="../audiolistener/" title="It is important to note that there is only one listener per context and that it isn't an AudioNode."><code>AudioListener</code></a> object, used for 3D spatialization.</dd> <dt>
<a href="../audiocontext/samplerate/" title="A floating point number."><code>AudioContext.sampleRate</code></a> <span class="inlineIndicator readOnly readOnlyInline" title="This value may not be changed.">Read only </span>
</dt> <dd>Returns a float representing the sample rate (in samples per second) used by all nodes in this context. The sample-rate of an <a href="../audiocontext/" title="An AudioContext can be a target of events, therefore it implements the EventTarget interface."><code>AudioContext</code></a> cannot be changed.</dd> <dt>
<a href="../audiocontext/state/" title="A DOMString. Possible values are:"><code>AudioContext.state</code></a> <span class="inlineIndicator readOnly readOnlyInline" title="This value may not be changed.">Read only </span>
</dt> <dd>Returns the current state of the <code>AudioContext</code>.</dd> <dt>
<a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/mozAudioChannelType" title="The readonly mozAudioChannelType property of the AudioContext interface can be used to set the audio channel that the sound playing in an audio context element will play in, on a Firefox OS device." target="_blank"><code>AudioContext.mozAudioChannelType</code></a>  <span class="inlineIndicator readOnly readOnlyInline" title="This value may not be changed.">Read only </span>
</dt> <dd>Used to return the audio channel that the sound playing in an <a href="../audiocontext/" title="An AudioContext can be a target of events, therefore it implements the EventTarget interface."><code>AudioContext</code></a> will play in, on a Firefox OS device.</dd> </dl> <h3 id="Event_handlers">Event handlers</h3> <dl> <dt><a href="../audiocontext/onstatechange/" title="Technical review completed. Editorial review completed."><code>AudioContext.onstatechange</code></a></dt> <dd>An event handler that runs when an event of type <code><a href="https://developer.mozilla.org/en-US/docs/Web/Events/statechange" title="/en-US/docs/Web/Events/statechange" target="_blank">statechange</a></code> has fired. This occurs when the <code>AudioContext</code>'s state changes, due to the calling of one of the state change methods (<a href="../audiocontext/suspend/" title="The suspend() method of the AudioContext Interface suspends the progression of time in the audio context, temporarily halting audio hardware access and reducing CPU/battery usage in the process — this is useful if you want an application to power down the audio hardware when it will not be using an audio context for a while."><code>AudioContext.suspend</code></a>, <a href="../audiocontext/resume/" title="The resume() method of the AudioContext Interface resumes the progression of time in an audio context that has previously been suspended."><code>AudioContext.resume</code></a>, or <a href="../audiocontext/close/" title="The close() method of the AudioContext Interface closes the audio context, releasing any system audio resources that it uses."><code>AudioContext.close</code></a>).</dd> </dl> <h2 id="Methods">Methods</h2> <p><em>Also implements methods from the interface </em><a href="../eventtarget/" title="EventTarget is an interface implemented by objects that can receive events and may have listeners for them."><code>EventTarget</code></a>.</p> <dl> <dt><a href="../audiocontext/close/" title="The close() method of the AudioContext Interface closes the audio context, releasing any system audio resources that it uses."><code>AudioContext.close()</code></a></dt> <dd>Closes the audio context, releasing any system audio resources that it uses.</dd> <dt><a href="../audiocontext/createbuffer/" title="An AudioBuffer."><code>AudioContext.createBuffer()</code></a></dt> <dd>Creates a new, empty <a href="../audiobuffer/" title="Objects of these types are designed to hold small audio snippets, typically less than 45 s. For longer sounds, objects implementing the MediaElementAudioSourceNode are more suitable. The buffer contains data in the following format:  non-interleaved IEEE754 32-bit linear PCM with a nominal range between -1 and +1, that is, 32bits floating point buffer, with each samples between -1.0 and 1.0. If the AudioBuffer has multiple channels, they are stored in separate buffer."><code>AudioBuffer</code></a> object, which can then be populated by data and played via an <a href="../audiobuffersourcenode/" title="The AudioBufferSourceNode interface represents an audio source consisting of in-memory audio data, stored in an AudioBuffer. It is an AudioNode that acts as an audio source."><code>AudioBufferSourceNode</code></a>.</dd> <dt><a href="../audiocontext/createconstantsource/" title="The createConstantSource() property of the AudioContext interface reates a ConstantSourceNode object, which is an audio source that continuously outputs a monaural (one-channel) sound signal whose samples all have the same value."><code>AudioContext.createConstantSource()</code></a></dt> <dd>Creates a <a href="../constantsourcenode/" title="The ConstantSourceNode interface—part of the Web Audio API—represents an audio source (based upon AudioScheduledSourceNode) whose output is single unchanging value. This makes it useful for cases in which you need a constant value coming in from an audio source. in addition, it can be used like a constructible AudioParam by automating the value of its offset or by connecting another node to it."><code>ConstantSourceNode</code></a> object, which is an audio source that continuously outputs a monaural (one-channel) sound signal whose samples all have the same value.</dd> <dt><a href="../audiocontext/createbuffersource/" title="An AudioBufferSourceNode."><code>AudioContext.createBufferSource()</code></a></dt> <dd>Creates an <a href="../audiobuffersourcenode/" title="The AudioBufferSourceNode interface represents an audio source consisting of in-memory audio data, stored in an AudioBuffer. It is an AudioNode that acts as an audio source."><code>AudioBufferSourceNode</code></a>, which can be used to play and manipulate audio data contained within an <a href="../audiobuffer/" title="Objects of these types are designed to hold small audio snippets, typically less than 45 s. For longer sounds, objects implementing the MediaElementAudioSourceNode are more suitable. The buffer contains data in the following format:  non-interleaved IEEE754 32-bit linear PCM with a nominal range between -1 and +1, that is, 32bits floating point buffer, with each samples between -1.0 and 1.0. If the AudioBuffer has multiple channels, they are stored in separate buffer."><code>AudioBuffer</code></a> object. <a href="../audiobuffer/" title="Objects of these types are designed to hold small audio snippets, typically less than 45 s. For longer sounds, objects implementing the MediaElementAudioSourceNode are more suitable. The buffer contains data in the following format:  non-interleaved IEEE754 32-bit linear PCM with a nominal range between -1 and +1, that is, 32bits floating point buffer, with each samples between -1.0 and 1.0. If the AudioBuffer has multiple channels, they are stored in separate buffer."><code>AudioBuffer</code></a>s are created using <a href="../audiocontext/createbuffer/" title="An AudioBuffer."><code>AudioContext.createBuffer</code></a> or returned by <a href="../audiocontext/decodeaudiodata/" title="This is the preferred method of creating an audio source for Web Audio API from an audio track."><code>AudioContext.decodeAudioData</code></a> when it successfully decodes an audio track.</dd> <dt><a href="../audiocontext/createmediaelementsource/" title="For more details about media element audio source nodes, check out the MediaElementAudioSourceNode reference page."><code>AudioContext.createMediaElementSource()</code></a></dt> <dd>Creates a <a href="../mediaelementaudiosourcenode/" title="A MediaElementSourceNode has no inputs and exactly one output, and is created using the AudioContext.createMediaElementSource method. The amount of channels in the output equals the number of channels of the audio referenced by the HTMLMediaElement used in the creation of the node, or is 1 if the HTMLMediaElement has no audio."><code>MediaElementAudioSourceNode</code></a> associated with an <a href="../htmlmediaelement/" title="The HTMLMediaElement interface adds to HTMLElement the properties and methods needed to support basic media-related capabilities that are common to audio and video. The HTMLVideoElement and HTMLAudioElement elements both inherit this interface."><code>HTMLMediaElement</code></a>. This can be used to play and manipulate audio from <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/video" title="Use the HTML &lt;video&gt; element to embed video content in a document." target="_blank"><code>&lt;video&gt;</code></a> or <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/audio" title="The HTML &lt;audio&gt; element is used to embed sound content in documents. It may contain one or more audio sources, represented using the src attribute or the &lt;source&gt; element; the browser will choose the most suitable one." target="_blank"><code>&lt;audio&gt;</code></a> elements.</dd> <dt><a href="../audiocontext/createmediastreamsource/" title="For more details about media stream audio source nodes, check out the MediaStreamAudioSourceNode reference page."><code>AudioContext.createMediaStreamSource()</code></a></dt> <dd>Creates a <a href="../mediastreamaudiosourcenode/" title="A MediaElementSourceNode has no inputs and exactly one output, and is created using the AudioContext.createMediaStreamSource method. The amount of channels in the output equals the number of channels in AudioMediaStreamTrack. If there is no valid media stream, then the number of output channels will be one silent channel."><code>MediaStreamAudioSourceNode</code></a> associated with a <a href="../mediastream/" title="The MediaStream interface represents a stream of media content. A stream consists of several tracks such as video or audio tracks. Each track is specified as an instance of MediaStreamTrack."><code>MediaStream</code></a> representing an audio stream which may come from the local computer microphone or other sources.</dd> <dt><a href="../audiocontext/createmediastreamdestination/" title="The MediaStream is created when the node is created and is accessible via the MediaStreamAudioDestinationNode's stream attribute. This stream can be used in a similar way as a MediaStream obtained via navigator.getUserMedia — it can, for example, be sent to a remote peer using the RTCPeerConnection addStream() method."><code>AudioContext.createMediaStreamDestination()</code></a></dt> <dd>Creates a <a href="../mediastreamaudiodestinationnode/" title="Inherits properties from its parent, AudioNode."><code>MediaStreamAudioDestinationNode</code></a> associated with a <a href="../mediastream/" title="The MediaStream interface represents a stream of media content. A stream consists of several tracks such as video or audio tracks. Each track is specified as an instance of MediaStreamTrack."><code>MediaStream</code></a> representing an audio stream which may be stored in a local file or sent to another computer.</dd> <dt><a href="../audiocontext/createscriptprocessor/" title="A ScriptProcessorNode."><code>AudioContext.createScriptProcessor()</code></a></dt> <dd>Creates a <a href="../scriptprocessornode/" title=""><code>ScriptProcessorNode</code></a>, which can be used for direct audio processing via JavaScript.</dd> <dt><a href="../audiocontext/createstereopanner/" title="A StereoPannerNode."><code>AudioContext.createStereoPanner()</code></a></dt> <dd>Creates a <a href="../stereopannernode/" title="The pan property takes a unitless value between -1 (full left pan) and 1 (full right pan). This interface was introduced as a much simpler way to apply a simple panning effect than having to use a full PannerNode."><code>StereoPannerNode</code></a>, which can be used to apply stereo panning to an audio source.</dd> <dt><a href="../audiocontext/createanalyser/" title="An AnalyserNode."><code>AudioContext.createAnalyser()</code></a></dt> <dd>Creates an <a href="../analysernode/" title="The AnalyserNode interface represents a node able to provide real-time frequency and time-domain analysis information. It is an AudioNode that passes the audio stream unchanged from the input to the output, but allows you to take the generated data, process it, and create audio visualizations."><code>AnalyserNode</code></a>, which can be used to expose audio time and frequency data and for example to create data visualisations.</dd> <dt><a href="../audiocontext/createbiquadfilter/" title="A BiquadFilterNode."><code>AudioContext.createBiquadFilter()</code></a></dt> <dd>Creates a <a href="../biquadfilternode/" title="The BiquadFilterNode interface represents a simple low-order filter, and is created using the AudioContext.createBiquadFilter() method. It is an AudioNode that can represent different kinds of filters, tone control devices, and graphic equalizers."><code>BiquadFilterNode</code></a>, which represents a second order filter configurable as several different common filter types: high-pass, low-pass, band-pass, etc.</dd> <dt><a href="../audiocontext/createchannelmerger/" title="A ChannelMergerNode."><code>AudioContext.createChannelMerger()</code></a></dt> <dd>Creates a <a href="../channelmergernode/" title=""><code>ChannelMergerNode</code></a>, which is used to combine channels from multiple audio streams into a single audio stream.</dd> <dt><a href="../audiocontext/createchannelsplitter/" title="A ChannelSplitterNode."><code>AudioContext.createChannelSplitter()</code></a></dt> <dd>Creates a <a href="../channelsplitternode/" title=""><code>ChannelSplitterNode</code></a>, which is used to access the individual channels of an audio stream and process them separately.</dd> <dt><a href="../audiocontext/createconvolver/" title="A ConvolverNode."><code>AudioContext.createConvolver()</code></a></dt> <dd>Creates a <a href="../convolvernode/" title="The ConvolverNode interface is an AudioNode that performs a Linear Convolution on a given AudioBuffer, often used to achieve a reverb effect. A ConvolverNode always has exactly one input and one output."><code>ConvolverNode</code></a>, which can be used to apply convolution effects to your audio graph, for example a reverberation effect.</dd> <dt><a href="../audiocontext/createdelay/" title="A DelayNode. The default DelayNode.delayTime if no parameter is passed to createDelay() is 0 seconds."><code>AudioContext.createDelay()</code></a></dt> <dd>Creates a <a href="../delaynode/" title=""><code>DelayNode</code></a>, which is used to delay the incoming audio signal by a certain amount. This node is also useful to create feedback loops in a Web Audio API graph.</dd> <dt><a href="../audiocontext/createdynamicscompressor/" title="Compression lowers the volume of the loudest parts of the signal and raises the volume of the softest parts. Overall, a louder, richer, and fuller sound can be achieved. It is especially important in games and musical applications where large numbers of individual sounds are played simultaneously, where you want to control the overall signal level and help avoid clipping (distorting) of the audio output."><code>AudioContext.createDynamicsCompressor()</code></a></dt> <dd>Creates a <a href="../dynamicscompressornode/" title="Inherits properties from its parent, AudioNode."><code>DynamicsCompressorNode</code></a>, which can be used to apply acoustic compression to an audio signal.</dd> <dt><a href="../audiocontext/creategain/" title="A GainNode."><code>AudioContext.createGain()</code></a></dt> <dd>Creates a <a href="../gainnode/" title="The gain is a unitless value, changing with time, that is multiplied to each corresponding sample of all input channels. If modified, the new gain is applied using a de-zippering algorithm in order to prevent unaesthetic 'clicks' from appearing in the resulting audio."><code>GainNode</code></a>, which can be used to control the overall volume of the audio graph.</dd> <dt><a href="../audiocontext/createiirfilter/" title="The createIIRFilter() method of the AudioContext interface creates an IIRFilterNode, which represents a general infinite impulse response (IIR) filter which can be configured to serve as various types of filter."><code>AudioContext.createIIRFilter()</code></a></dt> <dd>Creates an <a href="../iirfilternode/" title="The IIRFilterNode interface of the Web Audio API is a AudioNode processor which implements a general infinite impulse response (IIR)  filter; this type of filter can be used to implement tone control devices and graphic equalizers as well. It lets the parameters of the filter response be specified, so that it can be tuned as needed."><code>IIRFilterNode</code></a>, which represents a second order filter configurable as several different common filter types.</dd> <dt><a href="../audiocontext/createoscillator/" title="An OscillatorNode."><code>AudioContext.createOscillator()</code></a></dt> <dd>Creates an <a href="../oscillatornode/" title="The OscillatorNode interface represents a periodic waveform, such as a sine wave. It is an AudioScheduledSourceNode audio-processing module that causes a specified frequency of a given wave to be created—in effect, a constant tone."><code>OscillatorNode</code></a>, a source representing a periodic waveform. It basically generates a tone.</dd> <dt><a href="../audiocontext/createpanner/" title="The panner node is spatialized in relation to the AudioContext's AudioListener (defined by the AudioContext.listener attribute), which represents the position and orientation of the person listening to the audio."><code>AudioContext.createPanner()</code></a></dt> <dd>Creates a <a href="../pannernode/" title="A PannerNode always has exactly one input and one output: the input can be mono or stereo but the output is always stereo (2 channels); you can't have panning effects without at least two audio channels!"><code>PannerNode</code></a>, which is used to spatialise an incoming audio stream in 3D space.</dd> <dt><a href="../audiocontext/createperiodicwave/" title="A PeriodicWave."><code>AudioContext.createPeriodicWave()</code></a></dt> <dd>Creates a <a href="../periodicwave/" title="PeriodicWave has no inputs or outputs; it is used to define custom oscillators when calling OscillatorNode.setPeriodicWave(). The PeriodicWave itself is created/returned by AudioContext.createPeriodicWave()."><code>PeriodicWave</code></a>, used to define a periodic waveform that can be used to determine the output of an <a href="../oscillatornode/" title="The OscillatorNode interface represents a periodic waveform, such as a sine wave. It is an AudioScheduledSourceNode audio-processing module that causes a specified frequency of a given wave to be created—in effect, a constant tone."><code>OscillatorNode</code></a>.</dd> <dt><a href="../audiocontext/createwaveshaper/" title="A WaveShaperNode."><code>AudioContext.createWaveShaper()</code></a></dt> <dd>Creates a <a href="../waveshapernode/" title="A WaveShaperNode always has exactly one input and one output."><code>WaveShaperNode</code></a>, which is used to implement non-linear distortion effects.</dd> <dt><a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createAudioWorker" title="The documentation about this has not yet been written; please consider contributing!" target="_blank"><code>AudioContext.createAudioWorker()</code></a></dt> <dd>Creates an <a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioWorkerNode" title="The documentation about this has not yet been written; please consider contributing!" target="_blank"><code>AudioWorkerNode</code></a>, which can interact with a web worker thread to generate, process, or analyse audio directly. This was added to the spec on August 29 2014, and is not implemented in any browser yet.</dd> <dt><a href="../audiocontext/decodeaudiodata/" title="This is the preferred method of creating an audio source for Web Audio API from an audio track."><code>AudioContext.decodeAudioData()</code></a></dt> <dd>Asynchronously decodes audio file data contained in an <a href="https://developer.mozilla.org/en-US/docs/Web/API/ArrayBuffer" title="The documentation about this has not yet been written; please consider contributing!" target="_blank"><code>ArrayBuffer</code></a>. In this case, the ArrayBuffer is usually loaded from an <a href="../xmlhttprequest/" title="XMLHttpRequest is an API that provides client functionality for transferring data between a client and a server. It provides an easy way to retrieve data from a URL without having to do a full page refresh. This enables a Web page to update just a part of the page without disrupting what the user is doing."><code>XMLHttpRequest</code></a>'s <code>response</code> attribute after setting the <code>responseType</code> to <code>arraybuffer</code>. This method only works on complete files, not fragments of audio files.</dd> <dt><a href="../audiocontext/resume/" title="The resume() method of the AudioContext Interface resumes the progression of time in an audio context that has previously been suspended."><code>AudioContext.resume()</code></a></dt> <dd>Resumes the progression of time in an audio context that has previously been suspended.</dd> <dt><a href="../audiocontext/suspend/" title="The suspend() method of the AudioContext Interface suspends the progression of time in the audio context, temporarily halting audio hardware access and reducing CPU/battery usage in the process — this is useful if you want an application to power down the audio hardware when it will not be using an audio context for a while."><code>AudioContext.suspend()</code></a></dt> <dd>Suspends the progression of time in the audio context, temporarily halting audio hardware access and reducing CPU/battery usage in the process.</dd> </dl> <h2 id="Obsolete_methods">Obsolete methods</h2> <dl> <dt><a href="../audiocontext/createjavascriptnode/" title="This method is obsolete, and has been renamed to  AudioContext.createScriptProcessor. See also ScriptProcessorNode."><code>AudioContext.createJavaScriptNode()</code></a></dt> <dd>Creates a <a href="https://developer.mozilla.org/en-US/docs/Web/API/JavaScriptNode" title="The documentation about this has not yet been written; please consider contributing!" target="_blank"><code>JavaScriptNode</code></a>, used for direct audio processing via JavaScript. This method is obsolete, and has been replaced by <a href="../audiocontext/createscriptprocessor/" title="A ScriptProcessorNode."><code>AudioContext.createScriptProcessor()</code></a>.</dd> <dt><a href="../audiocontext/createwavetable/" title=""><code>AudioContext.createWaveTable()</code></a></dt> <dd>Creates a <a href="https://developer.mozilla.org/en-US/docs/Web/API/WaveTableNode" title="The documentation about this has not yet been written; please consider contributing!" target="_blank"><code>WaveTableNode</code></a>, used to define a periodic waveform. This method is obsolete, and has been replaced by <a href="../audiocontext/createperiodicwave/" title="A PeriodicWave."><code>AudioContext.createPeriodicWave()</code></a>.</dd> </dl> <h2 id="Examples">Examples</h2> <p>Basic audio context declaration:</p> <pre data-language="js">var audioCtx = new AudioContext();</pre> <p>Cross browser variant:</p> <pre data-language="js">var AudioContext = window.AudioContext || window.webkitAudioContext;
var audioCtx = new AudioContext();

var oscillatorNode = audioCtx.createOscillator();
var gainNode = audioCtx.createGain();
var finish = audioCtx.destination;
// etc.</pre> <h2 id="Specifications">Specifications</h2> <table class="standard-table"> <tbody> <tr> <th scope="col">Specification</th> <th scope="col">Status</th> <th scope="col">Comment</th> </tr> <tr> <td><a href="https://webaudio.github.io/web-audio-api/#the-audiocontext-interface" class="external" lang="en" hreflang="en" target="_blank">Web Audio API<br><small lang="en-US">The definition of 'AudioContext' in that specification.</small></a></td> <td><span class="spec-WD">Working Draft</span></td> <td> </td> </tr> </tbody> </table> <h2 id="Browser_compatibility">Browser compatibility</h2>  <div id="compat-desktop"> <table class="compat-table"> <tbody> <tr> <th>Feature</th> <th>Chrome</th> <th>Edge</th> <th>Firefox (Gecko)</th> <th>Internet Explorer</th> <th>Opera</th> <th>Safari (WebKit)</th> </tr> <tr> <td>Basic support</td> <td>10.0<span class="inlineIndicator prefixBox prefixBoxInline" title="prefix"><a href="https://developer.mozilla.org/en-US/docs/Web/Guide/Prefixes" title="The name of this feature is prefixed with 'webkit' as this browser considers it experimental" target="_blank">webkit</a></span><br> 35</td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> <td>
<a href="https://developer.mozilla.org/en-US/Firefox/Releases/25" title="Released on 2013-10-29." target="_blank">25.0</a> (25.0) </td> <td><span style="color: #f00;">No support</span></td> <td>15.0<span class="inlineIndicator prefixBox prefixBoxInline" title="prefix"><a href="https://developer.mozilla.org/en-US/docs/Web/Guide/Prefixes" title="The name of this feature is prefixed with 'webkit' as this browser considers it experimental" target="_blank">webkit</a></span><br> 22</td> <td>6.0<span class="inlineIndicator prefixBox prefixBoxInline" title="prefix"><a href="https://developer.mozilla.org/en-US/docs/Web/Guide/Prefixes" title="The name of this feature is prefixed with 'webkit' as this browser considers it experimental" target="_blank">webkit</a></span>
</td> </tr> <tr> <td><code>createStereoPanner()</code></td> <td>42.0</td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> <td>
<a href="https://developer.mozilla.org/en-US/Firefox/Releases/37" title="Released on 2015-04-07." target="_blank">37.0</a> (37.0) </td> <td><span style="color: #f00;">No support</span></td> <td><span style="color: #f00;">No support</span></td> <td><span style="color: #f00;">No support</span></td> </tr> <tr> <td>
<code>onstatechange</code>, <code>state</code>, <code>suspend()</code>, <code>resume()</code>
</td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> <td>
<a href="https://developer.mozilla.org/en-US/Firefox/Releases/40" title="Released on 2015-08-11." target="_blank">40.0</a> (40.0)</td> <td><span style="color: #f00;">No support</span></td> <td><span style="color: #f00;">No support</span></td> <td><span style="color: #f00;">No support</span></td> </tr> <tr> <td><code>createConstantSource()</code></td> <td>56.0</td> <td><span style="color: #f00;">No support</span></td> <td>
<a href="https://developer.mozilla.org/en-US/Firefox/Releases/52" title="Released on 2017-03-07." target="_blank">52</a> (52)</td> <td><span style="color: #f00;">No support</span></td> <td>43</td> <td><span style="color: #f00;">No support</span></td> </tr> <tr> <td>Unprefixed</td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> </table> </div> <div id="compat-mobile"> <table class="compat-table"> <tbody> <tr> <th>Feature</th> <th>Android Webview</th> <th>Edge</th> <th>Firefox Mobile (Gecko)</th> <th>Firefox OS</th> <th>IE Mobile</th> <th>Opera Mobile</th> <th>Safari Mobile</th> <th>Chrome for Android</th> </tr> <tr> <td>Basic support</td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> <td>
<a href="https://developer.mozilla.org/en-US/Firefox/Releases/37" title="Released on 2015-04-07." target="_blank">37.0</a> (37.0) </td> <td>2.2</td> <td><span style="color: #f00;">No support</span></td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> <td><span style="color: #f00;">No support</span></td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> </tr> <tr> <td><code>createStereoPanner()</code></td> <td>42.0</td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> <td><span style="color: #f00;">No support</span></td> <td><span style="color: #f00;">No support</span></td> <td><span style="color: #f00;">No support</span></td> <td>42.0</td> </tr> <tr> <td>
<code>onstatechange</code>, <code>state</code>, <code>suspend()</code>, <code>resume()</code>
</td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> <td><span style="color: #f00;">No support</span></td> <td><span style="color: #f00;">No support</span></td> <td><span style="color: #f00;">No support</span></td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> </tr> <tr> <td><code>createConstantSource()</code></td> <td>56.0</td> <td><span style="color: #f00;">No support</span></td> <td>52.0 (52)</td> <td><span style="color: #f00;">No support</span></td> <td><span style="color: #f00;">No support</span></td> <td><span style="color: #f00;">No support</span></td> <td><span style="color: #f00;">No support</span></td> <td>56.0</td> </tr> <tr> <td>Unprefixed</td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> <td><span title="Compatibility unknown; please update this." style="color: rgb(255, 153, 0);">?</span></td> <td><span title="Compatibility unknown; please update this." style="color: rgb(255, 153, 0);">?</span></td> <td><span title="Compatibility unknown; please update this." style="color: rgb(255, 153, 0);">?</span></td> <td>43</td> <td><span title="Compatibility unknown; please update this." style="color: rgb(255, 153, 0);">?</span></td> <td><span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span></td> </tr> </tbody> </table> </div> <h2 id="See_also">See also</h2> <ul style="margin-left: 40px;"> <li><a href="https://developer.mozilla.org/en-US/docs/Web_Audio_API/Using_Web_Audio_API" target="_blank">Using the Web Audio API</a></li> <li><a href="../offlineaudiocontext/" title="The OfflineAudioContext interface is an AudioContext interface representing an audio-processing graph built from linked together AudioNodes. In contrast with a standard AudioContext, an OfflineAudioContext doesn't render the audio to the device hardware; instead, it generates it, as fast as it can, and outputs the result to an AudioBuffer."><code>OfflineAudioContext</code></a></li> </ul>
<div class="_attribution">
  <p class="_attribution-p">
    <a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioContext%24edit" class="_attribution-link" target="_blank">Edit this page on MDN</a>
  </p>
</div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2005–2017 Mozilla Developer Network and individual contributors.<br>Licensed under the Creative Commons Attribution-ShareAlike License v2.5 or later.<br>
    <a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioContext" class="_attribution-link" target="_blank">https://developer.mozilla.org/en-US/docs/Web/API/AudioContext</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
