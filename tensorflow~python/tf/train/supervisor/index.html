
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>tf.train.Supervisor - TensorFlow Python - W3cubDocs</title>
  
  <meta name="description" content=" See the guide&#58; Training &#62; Distributed execution ">
  <meta name="keywords" content="tf, train, supervisor, -, tensorflow, python, tensorflow~python">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/tensorflow~python/tf/train/supervisor/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/tensorflow~python.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~python/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Python</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _tensorflow">
				
<h1 itemprop="name" class="devsite-page-title"> tf.train.Supervisor </h1>    <h3 id="class_tftrainsupervisor"><code>class tf.train.Supervisor</code></h3> <p>See the guide: <a href="https://www.tensorflow.org/api_guides/python/train#Distributed_execution" target="_blank">Training &gt; Distributed execution</a></p> <p>A training helper that checkpoints models and computes summaries.</p> <p>The Supervisor is a small wrapper around a <code>Coordinator</code>, a <code>Saver</code>, and a <code>SessionManager</code> that takes care of common needs of TensorFlow training programs.</p> <h4 id="use_for_a_single_program">Use for a single program</h4> <pre class="prettyprint lang-python" data-language="python">with tf.Graph().as_default():
  ...add operations to the graph...
  # Create a Supervisor that will checkpoint the model in '/tmp/mydir'.
  sv = Supervisor(logdir='/tmp/mydir')
  # Get a TensorFlow session managed by the supervisor.
  with sv.managed_session(FLAGS.master) as sess:
    # Use the session to train the graph.
    while not sv.should_stop():
      sess.run(&lt;my_train_op&gt;)
</pre> <p>Within the <code>with sv.managed_session()</code> block all variables in the graph have been initialized. In addition, a few services have been started to checkpoint the model and add summaries to the event log.</p> <p>If the program crashes and is restarted, the managed session automatically reinitialize variables from the most recent checkpoint.</p> <p>The supervisor is notified of any exception raised by one of the services. After an exception is raised, <code>should_stop()</code> returns <code>True</code>. In that case the training loop should also stop. This is why the training loop has to check for <code>sv.should_stop()</code>.</p> <p>Exceptions that indicate that the training inputs have been exhausted, <code>tf.errors.OutOfRangeError</code>, also cause <code>sv.should_stop()</code> to return <code>True</code> but are not re-raised from the <code>with</code> block: they indicate a normal termination.</p> <h4 id="use_for_multiple_replicas">Use for multiple replicas</h4> <p>To train with replicas you deploy the same program in a <code>Cluster</code>. One of the tasks must be identified as the <em>chief</em>: the task that handles initialization, checkpoints, summaries, and recovery. The other tasks depend on the <em>chief</em> for these services.</p> <p>The only change you have to do to the single program code is to indicate if the program is running as the <em>chief</em>.</p> <pre class="prettyprint lang-python" data-language="python"># Choose a task as the chief. This could be based on server_def.task_index,
# or job_def.name, or job_def.tasks. It's entirely up to the end user.
# But there can be only one *chief*.
is_chief = (server_def.task_index == 0)
server = tf.train.Server(server_def)

with tf.Graph().as_default():
  ...add operations to the graph...
  # Create a Supervisor that uses log directory on a shared file system.
  # Indicate if you are the 'chief'
  sv = Supervisor(logdir='/shared_directory/...', is_chief=is_chief)
  # Get a Session in a TensorFlow server on the cluster.
  with sv.managed_session(server.target) as sess:
    # Use the session to train the graph.
    while not sv.should_stop():
      sess.run(&lt;my_train_op&gt;)
</pre> <p>In the <em>chief</em> task, the <code>Supervisor</code> works exactly as in the first example above. In the other tasks <code>sv.managed_session()</code> waits for the Model to have been initialized before returning a session to the training code. The non-chief tasks depend on the chief task for initializing the model.</p> <p>If one of the tasks crashes and restarts, <code>managed_session()</code> checks if the Model is initialized. If yes, it just creates a session and returns it to the training code that proceeds normally. If the model needs to be initialized, the chief task takes care of reinitializing it; the other tasks just wait for the model to have been initialized.</p> <p>NOTE: This modified program still works fine as a single program. The single program marks itself as the chief.</p> <h4 id="what_master_string_to_use">What <code>master</code> string to use</h4> <p>Whether you are running on your machine or in the cluster you can use the following values for the --master flag:</p> <ul> <li> <p>Specifying <code>''</code> requests an in-process session that does not use RPC.</p> </li> <li> <p>Specifying <code>'local'</code> requests a session that uses the RPC-based "Master interface" to run TensorFlow programs. See <a href="../server/#create_local_server"><code>tf.train.Server.create_local_server</code></a> for details.</p> </li> <li> <p>Specifying <code>'grpc://hostname:port'</code> requests a session that uses the RPC interface to a specific host, and also allows the in-process master to access remote tensorflow workers. Often, it is appropriate to pass <code>server.target</code> (for some <code>tf.train.Server</code> named `server).</p> </li> </ul> <h4 id="advanced_use">Advanced use</h4> <h5 id="launching_additional_services">Launching additional services</h5> <p><code>managed_session()</code> launches the Checkpoint and Summary services (threads). If you need more services to run you can simply launch them in the block controlled by <code>managed_session()</code>.</p> <p>Example: Start a thread to print losses. We want this thread to run every 60 seconds, so we launch it with <code>sv.loop()</code>.</p> <pre class="prettyprint notranslate" translate="no" data-language="python">...
sv = Supervisor(logdir='/tmp/mydir')
with sv.managed_session(FLAGS.master) as sess:
  sv.loop(60, print_loss, (sess, ))
  while not sv.should_stop():
    sess.run(my_train_op)
</pre> <h5 id="launching_fewer_services">Launching fewer services</h5> <p><code>managed_session()</code> launches the "summary" and "checkpoint" threads which use either the optionally <code>summary_op</code> and <code>saver</code> passed to the constructor, or default ones created automatically by the supervisor. If you want to run your own summary and checkpointing logic, disable these services by passing <code>None</code> to the <code>summary_op</code> and <code>saver</code> parameters.</p> <p>Example: Create summaries manually every 100 steps in the chief.</p> <pre class="prettyprint notranslate" translate="no" data-language="python"># Create a Supervisor with no automatic summaries.
sv = Supervisor(logdir='/tmp/mydir', is_chief=is_chief, summary_op=None)
# As summary_op was None, managed_session() does not start the
# summary thread.
with sv.managed_session(FLAGS.master) as sess:
  for step in xrange(1000000):
    if sv.should_stop():
      break
    if is_chief and step % 100 == 0:
      # Create the summary every 100 chief steps.
      sv.summary_computed(sess, sess.run(my_summary_op))
    else:
      # Train normally
      sess.run(my_train_op)
</pre> <h5 id="custom_model_initialization">Custom model initialization</h5> <p><code>managed_session()</code> only supports initializing the model by running an <code>init_op</code> or restoring from the latest checkpoint. If you have special initialization needs, see how to specify a <code>local_init_op</code> when creating the supervisor. You can also use the <code>SessionManager</code> directly to create a session and check if it could be initialized automatically.</p> <h2 id="properties">Properties</h2> <h3 id="coord"><code>coord</code></h3> <p>Return the Coordinator used by the Supervisor.</p> <p>The Coordinator can be useful if you want to run multiple threads during your training.</p> <h4 id="returns">Returns:</h4> <p>A Coordinator object.</p> <h3 id="global_step"><code>global_step</code></h3> <p>Return the global_step Tensor used by the supervisor.</p> <h4 id="returns_1">Returns:</h4> <p>An integer Tensor for the global_step.</p> <h3 id="init_feed_dict"><code>init_feed_dict</code></h3> <p>Return the feed dictionary used when evaluating the <code>init_op</code>.</p> <h4 id="returns_2">Returns:</h4> <p>A feed dictionary or <code>None</code>.</p> <h3 id="init_op"><code>init_op</code></h3> <p>Return the Init Op used by the supervisor.</p> <h4 id="returns_3">Returns:</h4> <p>An Op or <code>None</code>.</p> <h3 id="is_chief"><code>is_chief</code></h3> <p>Return True if this is a chief supervisor.</p> <h4 id="returns_4">Returns:</h4> <p>A bool.</p> <h3 id="ready_for_local_init_op"><code>ready_for_local_init_op</code></h3> <h3 id="ready_op"><code>ready_op</code></h3> <p>Return the Ready Op used by the supervisor.</p> <h4 id="returns_5">Returns:</h4> <p>An Op or <code>None</code>.</p> <h3 id="save_model_secs"><code>save_model_secs</code></h3> <p>Return the delay between checkpoints.</p> <h4 id="returns_6">Returns:</h4> <p>A timestamp.</p> <h3 id="save_path"><code>save_path</code></h3> <p>Return the save path used by the supervisor.</p> <h4 id="returns_7">Returns:</h4> <p>A string.</p> <h3 id="save_summaries_secs"><code>save_summaries_secs</code></h3> <p>Return the delay between summary computations.</p> <h4 id="returns_8">Returns:</h4> <p>A timestamp.</p> <h3 id="saver"><code>saver</code></h3> <p>Return the Saver used by the supervisor.</p> <h4 id="returns_9">Returns:</h4> <p>A Saver object.</p> <h3 id="session_manager"><code>session_manager</code></h3> <p>Return the SessionManager used by the Supervisor.</p> <h4 id="returns_10">Returns:</h4> <p>A SessionManager object.</p> <h3 id="summary_op"><code>summary_op</code></h3> <p>Return the Summary Tensor used by the chief supervisor.</p> <h4 id="returns_11">Returns:</h4> <p>A string Tensor for the summary or <code>None</code>.</p> <h3 id="summary_writer"><code>summary_writer</code></h3> <p>Return the SummaryWriter used by the chief supervisor.</p> <h4 id="returns_12">Returns:</h4> <p>A SummaryWriter.</p> <h2 id="methods">Methods</h2> <h3 id="Loop"><code>Loop(timer_interval_secs, target, args=None, kwargs=None)</code></h3> <p>Start a LooperThread that calls a function periodically.</p> <p>If <code>timer_interval_secs</code> is None the thread calls <code>target(*args, **kwargs)</code> repeatedly. Otherwise it calls it every <code>timer_interval_secs</code> seconds. The thread terminates when a stop is requested.</p> <p>The started thread is added to the list of threads managed by the supervisor so it does not need to be passed to the <code>stop()</code> method.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code>timer_interval_secs</code></b>: Number. Time boundaries at which to call <code>target</code>.</li> <li>
<b><code>target</code></b>: A callable object.</li> <li>
<b><code>args</code></b>: Optional arguments to pass to <code>target</code> when calling it.</li> <li>
<b><code>kwargs</code></b>: Optional keyword arguments to pass to <code>target</code> when calling it.</li> </ul> <h4 id="returns_13">Returns:</h4> <p>The started thread.</p> <h3 id="PrepareSession"><code>PrepareSession(master='', config=None, wait_for_checkpoint=False, max_wait_secs=7200, start_standard_services=True)</code></h3> <p>Make sure the model is ready to be used.</p> <p>Create a session on 'master', recovering or initializing the model as needed, or wait for a session to be ready. If running as the chief and <code>start_standard_service</code> is set to True, also call the session manager to start the standard services.</p> <h4 id="args_1">Args:</h4> <ul> <li>
<b><code>master</code></b>: name of the TensorFlow master to use. See the <code>tf.Session</code> constructor for how this is interpreted.</li> <li>
<b><code>config</code></b>: Optional ConfigProto proto used to configure the session, which is passed as-is to create the session.</li> <li>
<b><code>wait_for_checkpoint</code></b>: Whether we should wait for the availability of a checkpoint before creating Session. Defaults to False.</li> <li>
<b><code>max_wait_secs</code></b>: Maximum time to wait for the session to become available.</li> <li>
<b><code>start_standard_services</code></b>: Whether to start the standard services and the queue runners.</li> </ul> <h4 id="returns_14">Returns:</h4> <p>A Session object that can be used to drive the model.</p> <h3 id="RequestStop"><code>RequestStop(ex=None)</code></h3> <p>Request that the coordinator stop the threads.</p> <p>See <code>Coordinator.request_stop()</code>.</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code>ex</code></b>: Optional <code>Exception</code>, or Python <code>exc_info</code> tuple as returned by <code>sys.exc_info()</code>. If this is the first call to <code>request_stop()</code> the corresponding exception is recorded and re-raised from <code>join()</code>.</li> </ul> <h3 id="ShouldStop"><code>ShouldStop()</code></h3> <p>Check if the coordinator was told to stop.</p> <p>See <code>Coordinator.should_stop()</code>.</p> <h4 id="returns_15">Returns:</h4> <p>True if the coordinator was told to stop, False otherwise.</p> <h3 id="StartQueueRunners"><code>StartQueueRunners(sess, queue_runners=None)</code></h3> <p>Start threads for <code>QueueRunners</code>.</p> <p>Note that the queue runners collected in the graph key <code>QUEUE_RUNNERS</code> are already started automatically when you create a session with the supervisor, so unless you have non-collected queue runners to start you do not need to call this explicitly.</p> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code>sess</code></b>: A <code>Session</code>.</li> <li>
<b><code>queue_runners</code></b>: A list of <code>QueueRunners</code>. If not specified, we'll use the list of queue runners gathered in the graph under the key <code>GraphKeys.QUEUE_RUNNERS</code>.</li> </ul> <h4 id="returns_16">Returns:</h4> <p>The list of threads started for the <code>QueueRunners</code>.</p> <h3 id="StartStandardServices"><code>StartStandardServices(sess)</code></h3> <p>Start the standard services for 'sess'.</p> <p>This starts services in the background. The services started depend on the parameters to the constructor and may include:</p> <ul> <li>A Summary thread computing summaries every save_summaries_secs.</li> <li>A Checkpoint thread saving the model every save_model_secs.</li> <li>A StepCounter thread measure step time.</li> </ul> <h4 id="args_4">Args:</h4> <ul> <li>
<b><code>sess</code></b>: A Session.</li> </ul> <h4 id="returns_17">Returns:</h4> <p>A list of threads that are running the standard services. You can use the Supervisor's Coordinator to join these threads with: sv.coord.Join(<list of threads>)</list></p> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code>RuntimeError</code></b>: If called with a non-chief Supervisor.</li> <li>
<b><code>ValueError</code></b>: If not <code>logdir</code> was passed to the constructor as the services need a log directory.</li> </ul> <h3 id="Stop"><code>Stop(threads=None, close_summary_writer=True)</code></h3> <p>Stop the services and the coordinator.</p> <p>This does not close the session.</p> <h4 id="args_5">Args:</h4> <ul> <li>
<b><code>threads</code></b>: Optional list of threads to join with the coordinator. If <code>None</code>, defaults to the threads running the standard services, the threads started for <code>QueueRunners</code>, and the threads started by the <code>loop()</code> method. To wait on additional threads, pass the list in this parameter.</li> <li>
<b><code>close_summary_writer</code></b>: Whether to close the <code>summary_writer</code>. Defaults to <code>True</code> if the summary writer was created by the supervisor, <code>False</code> otherwise.</li> </ul> <h3 id="StopOnException"><code>StopOnException()</code></h3> <p>Context handler to stop the supervisor when an exception is raised.</p> <p>See <code>Coordinator.stop_on_exception()</code>.</p> <h4 id="returns_18">Returns:</h4> <p>A context handler.</p> <h3 id="SummaryComputed"><code>SummaryComputed(sess, summary, global_step=None)</code></h3> <p>Indicate that a summary was computed.</p> <h4 id="args_6">Args:</h4> <ul> <li>
<b><code>sess</code></b>: A <code>Session</code> object.</li> <li>
<b><code>summary</code></b>: A Summary proto, or a string holding a serialized summary proto.</li> <li>
<b><code>global_step</code></b>: Int. global step this summary is associated with. If <code>None</code>, it will try to fetch the current step.</li> </ul> <h4 id="raises_1">Raises:</h4> <ul> <li>
<b><code>TypeError</code></b>: if 'summary' is not a Summary proto or a string.</li> <li>
<b><code>RuntimeError</code></b>: if the Supervisor was created without a <code>logdir</code>.</li> </ul> <h3 id="WaitForStop"><code>WaitForStop()</code></h3> <p>Block waiting for the coordinator to stop.</p> <h3 id="__init__"><code>__init__(graph=None, ready_op=USE_DEFAULT, ready_for_local_init_op=USE_DEFAULT, is_chief=True, init_op=USE_DEFAULT, init_feed_dict=None, local_init_op=USE_DEFAULT, logdir=None, summary_op=USE_DEFAULT, saver=USE_DEFAULT, global_step=USE_DEFAULT, save_summaries_secs=120, save_model_secs=600, recovery_wait_secs=30, stop_grace_secs=120, checkpoint_basename='model.ckpt', session_manager=None, summary_writer=USE_DEFAULT, init_fn=None)</code></h3> <p>Create a <code>Supervisor</code>.</p> <h4 id="args_7">Args:</h4> <ul> <li>
<b><code>graph</code></b>: A <code>Graph</code>. The graph that the model will use. Defaults to the default <code>Graph</code>. The supervisor may add operations to the graph before creating a session, but the graph should not be modified by the caller after passing it to the supervisor.</li> <li>
<b><code>ready_op</code></b>: 1-D string <code>Tensor</code>. This tensor is evaluated by supervisors in <code>prepare_or_wait_for_session()</code> to check if the model is ready to use. The model is considered ready if it returns an empty array. Defaults to the tensor returned from <code>tf.report_uninitialized_variables()</code> If <code>None</code>, the model is not checked for readiness.</li> <li>
<b><code>ready_for_local_init_op</code></b>: 1-D string <code>Tensor</code>. This tensor is evaluated by supervisors in <code>prepare_or_wait_for_session()</code> to check if the model is ready to run the local_init_op. The model is considered ready if it returns an empty array. Defaults to the tensor returned from <code>tf.report_uninitialized_variables(tf.global_variables())</code>. If <code>None</code>, the model is not checked for readiness before running local_init_op.</li> <li>
<b><code>is_chief</code></b>: If True, create a chief supervisor in charge of initializing and restoring the model. If False, create a supervisor that relies on a chief supervisor for inits and restore.</li> <li>
<b><code>init_op</code></b>: <code>Operation</code>. Used by chief supervisors to initialize the model when it can not be recovered. Defaults to an <code>Operation</code> that initializes all variables. If <code>None</code>, no initialization is done automatically unless you pass a value for <code>init_fn</code>, see below.</li> <li>
<b><code>init_feed_dict</code></b>: A dictionary that maps <code>Tensor</code> objects to feed values. This feed dictionary will be used when <code>init_op</code> is evaluated.</li> <li>
<b><code>local_init_op</code></b>: <code>Operation</code>. Used by all supervisors to run initializations that should run for every new supervisor instance. By default these are table initializers and initializers for local variables. If <code>None</code>, no further per supervisor-instance initialization is done automatically.</li> <li>
<b><code>logdir</code></b>: A string. Optional path to a directory where to checkpoint the model and log events for the visualizer. Used by chief supervisors. The directory will be created if it does not exist.</li> <li>
<b><code>summary_op</code></b>: An <code>Operation</code> that returns a Summary for the event logs. Used by chief supervisors if a <code>logdir</code> was specified. Defaults to the operation returned from summary.merge_all(). If <code>None</code>, summaries are not computed automatically.</li> <li>
<b><code>saver</code></b>: A Saver object. Used by chief supervisors if a <code>logdir</code> was specified. Defaults to the saved returned by Saver(). If <code>None</code>, the model is not saved automatically.</li> <li>
<b><code>global_step</code></b>: An integer Tensor of size 1 that counts steps. The value from 'global_step' is used in summaries and checkpoint filenames. Default to the op named 'global_step' in the graph if it exists, is of rank 1, size 1, and of type tf.int32 or tf.int64. If <code>None</code> the global step is not recorded in summaries and checkpoint files. Used by chief supervisors if a <code>logdir</code> was specified.</li> <li>
<b><code>save_summaries_secs</code></b>: Number of seconds between the computation of summaries for the event log. Defaults to 120 seconds. Pass 0 to disable summaries.</li> <li>
<b><code>save_model_secs</code></b>: Number of seconds between the creation of model checkpoints. Defaults to 600 seconds. Pass 0 to disable checkpoints.</li> <li>
<b><code>recovery_wait_secs</code></b>: Number of seconds between checks that the model is ready. Used by supervisors when waiting for a chief supervisor to initialize or restore the model. Defaults to 30 seconds.</li> <li>
<b><code>stop_grace_secs</code></b>: Grace period, in seconds, given to running threads to stop when <code>stop()</code> is called. Defaults to 120 seconds.</li> <li>
<b><code>checkpoint_basename</code></b>: The basename for checkpoint saving.</li> <li>
<b><code>session_manager</code></b>: <code>SessionManager</code>, which manages Session creation and recovery. If it is <code>None</code>, a default <code>SessionManager</code> will be created with the set of arguments passed in for backwards compatibility.</li> <li>
<b><code>summary_writer</code></b>: <code>SummaryWriter</code> to use or <code>USE_DEFAULT</code>. Can be <code>None</code> to indicate that no summaries should be written.</li> <li>
<b><code>init_fn</code></b>: Optional callable used to initialize the model. Called after the optional <code>init_op</code> is called. The callable must accept one argument, the session being initialized.</li> </ul> <h4 id="returns_19">Returns:</h4> <p>A <code>Supervisor</code>.</p> <h3 id="loop"><code>loop(timer_interval_secs, target, args=None, kwargs=None)</code></h3> <p>Start a LooperThread that calls a function periodically.</p> <p>If <code>timer_interval_secs</code> is None the thread calls <code>target(*args, **kwargs)</code> repeatedly. Otherwise it calls it every <code>timer_interval_secs</code> seconds. The thread terminates when a stop is requested.</p> <p>The started thread is added to the list of threads managed by the supervisor so it does not need to be passed to the <code>stop()</code> method.</p> <h4 id="args_8">Args:</h4> <ul> <li>
<b><code>timer_interval_secs</code></b>: Number. Time boundaries at which to call <code>target</code>.</li> <li>
<b><code>target</code></b>: A callable object.</li> <li>
<b><code>args</code></b>: Optional arguments to pass to <code>target</code> when calling it.</li> <li>
<b><code>kwargs</code></b>: Optional keyword arguments to pass to <code>target</code> when calling it.</li> </ul> <h4 id="returns_20">Returns:</h4> <p>The started thread.</p> <h3 id="managed_session"><code>managed_session(*args, **kwds)</code></h3> <p>Returns a context manager for a managed session.</p> <p>This context manager creates and automatically recovers a session. It optionally starts the standard services that handle checkpoints and summaries. It monitors exceptions raised from the <code>with</code> block or from the services and stops the supervisor as needed.</p> <p>The context manager is typically used as follows:</p> <pre class="prettyprint lang-python" data-language="python">def train():
  sv = tf.train.Supervisor(...)
  with sv.managed_session(&lt;master&gt;) as sess:
    for step in xrange(..):
      if sv.should_stop():
        break
      sess.run(&lt;my training op&gt;)
      ...do other things needed at each training step...
</pre> <p>An exception raised from the <code>with</code> block or one of the service threads is raised again when the block exits. This is done after stopping all threads and closing the session. For example, an <code>AbortedError</code> exception, raised in case of preemption of one of the workers in a distributed model, is raised again when the block exits.</p> <p>If you want to retry the training loop in case of preemption you can do it as follows:</p> <pre class="prettyprint lang-python" data-language="python">def main(...):
  while True
    try:
      train()
    except tf.errors.Aborted:
      pass
</pre> <p>As a special case, exceptions used for control flow, such as <code>OutOfRangeError</code> which reports that input queues are exhausted, are not raised again from the <code>with</code> block: they indicate a clean termination of the training loop and are considered normal termination.</p> <h4 id="args_9">Args:</h4> <ul> <li>
<b><code>master</code></b>: name of the TensorFlow master to use. See the <code>tf.Session</code> constructor for how this is interpreted.</li> <li>
<b><code>config</code></b>: Optional <code>ConfigProto</code> proto used to configure the session. Passed as-is to create the session.</li> <li>
<b><code>start_standard_services</code></b>: Whether to start the standard services, such as checkpoint, summary and step counter.</li> <li>
<b><code>close_summary_writer</code></b>: Whether to close the summary writer when closing the session. Defaults to True.</li> </ul> <h4 id="returns_21">Returns:</h4> <p>A context manager that yields a <code>Session</code> restored from the latest checkpoint or initialized from scratch if not checkpoint exists. The session is closed when the <code>with</code> block exits.</p> <h3 id="prepare_or_wait_for_session"><code>prepare_or_wait_for_session(master='', config=None, wait_for_checkpoint=False, max_wait_secs=7200, start_standard_services=True)</code></h3> <p>Make sure the model is ready to be used.</p> <p>Create a session on 'master', recovering or initializing the model as needed, or wait for a session to be ready. If running as the chief and <code>start_standard_service</code> is set to True, also call the session manager to start the standard services.</p> <h4 id="args_10">Args:</h4> <ul> <li>
<b><code>master</code></b>: name of the TensorFlow master to use. See the <code>tf.Session</code> constructor for how this is interpreted.</li> <li>
<b><code>config</code></b>: Optional ConfigProto proto used to configure the session, which is passed as-is to create the session.</li> <li>
<b><code>wait_for_checkpoint</code></b>: Whether we should wait for the availability of a checkpoint before creating Session. Defaults to False.</li> <li>
<b><code>max_wait_secs</code></b>: Maximum time to wait for the session to become available.</li> <li>
<b><code>start_standard_services</code></b>: Whether to start the standard services and the queue runners.</li> </ul> <h4 id="returns_22">Returns:</h4> <p>A Session object that can be used to drive the model.</p> <h3 id="request_stop"><code>request_stop(ex=None)</code></h3> <p>Request that the coordinator stop the threads.</p> <p>See <code>Coordinator.request_stop()</code>.</p> <h4 id="args_11">Args:</h4> <ul> <li>
<b><code>ex</code></b>: Optional <code>Exception</code>, or Python <code>exc_info</code> tuple as returned by <code>sys.exc_info()</code>. If this is the first call to <code>request_stop()</code> the corresponding exception is recorded and re-raised from <code>join()</code>.</li> </ul> <h3 id="should_stop"><code>should_stop()</code></h3> <p>Check if the coordinator was told to stop.</p> <p>See <code>Coordinator.should_stop()</code>.</p> <h4 id="returns_23">Returns:</h4> <p>True if the coordinator was told to stop, False otherwise.</p> <h3 id="start_queue_runners"><code>start_queue_runners(sess, queue_runners=None)</code></h3> <p>Start threads for <code>QueueRunners</code>.</p> <p>Note that the queue runners collected in the graph key <code>QUEUE_RUNNERS</code> are already started automatically when you create a session with the supervisor, so unless you have non-collected queue runners to start you do not need to call this explicitly.</p> <h4 id="args_12">Args:</h4> <ul> <li>
<b><code>sess</code></b>: A <code>Session</code>.</li> <li>
<b><code>queue_runners</code></b>: A list of <code>QueueRunners</code>. If not specified, we'll use the list of queue runners gathered in the graph under the key <code>GraphKeys.QUEUE_RUNNERS</code>.</li> </ul> <h4 id="returns_24">Returns:</h4> <p>The list of threads started for the <code>QueueRunners</code>.</p> <h3 id="start_standard_services"><code>start_standard_services(sess)</code></h3> <p>Start the standard services for 'sess'.</p> <p>This starts services in the background. The services started depend on the parameters to the constructor and may include:</p> <ul> <li>A Summary thread computing summaries every save_summaries_secs.</li> <li>A Checkpoint thread saving the model every save_model_secs.</li> <li>A StepCounter thread measure step time.</li> </ul> <h4 id="args_13">Args:</h4> <ul> <li>
<b><code>sess</code></b>: A Session.</li> </ul> <h4 id="returns_25">Returns:</h4> <p>A list of threads that are running the standard services. You can use the Supervisor's Coordinator to join these threads with: sv.coord.Join(<list of threads>)</list></p> <h4 id="raises_2">Raises:</h4> <ul> <li>
<b><code>RuntimeError</code></b>: If called with a non-chief Supervisor.</li> <li>
<b><code>ValueError</code></b>: If not <code>logdir</code> was passed to the constructor as the services need a log directory.</li> </ul> <h3 id="stop"><code>stop(threads=None, close_summary_writer=True)</code></h3> <p>Stop the services and the coordinator.</p> <p>This does not close the session.</p> <h4 id="args_14">Args:</h4> <ul> <li>
<b><code>threads</code></b>: Optional list of threads to join with the coordinator. If <code>None</code>, defaults to the threads running the standard services, the threads started for <code>QueueRunners</code>, and the threads started by the <code>loop()</code> method. To wait on additional threads, pass the list in this parameter.</li> <li>
<b><code>close_summary_writer</code></b>: Whether to close the <code>summary_writer</code>. Defaults to <code>True</code> if the summary writer was created by the supervisor, <code>False</code> otherwise.</li> </ul> <h3 id="stop_on_exception"><code>stop_on_exception()</code></h3> <p>Context handler to stop the supervisor when an exception is raised.</p> <p>See <code>Coordinator.stop_on_exception()</code>.</p> <h4 id="returns_26">Returns:</h4> <p>A context handler.</p> <h3 id="summary_computed"><code>summary_computed(sess, summary, global_step=None)</code></h3> <p>Indicate that a summary was computed.</p> <h4 id="args_15">Args:</h4> <ul> <li>
<b><code>sess</code></b>: A <code>Session</code> object.</li> <li>
<b><code>summary</code></b>: A Summary proto, or a string holding a serialized summary proto.</li> <li>
<b><code>global_step</code></b>: Int. global step this summary is associated with. If <code>None</code>, it will try to fetch the current step.</li> </ul> <h4 id="raises_3">Raises:</h4> <ul> <li>
<b><code>TypeError</code></b>: if 'summary' is not a Summary proto or a string.</li> <li>
<b><code>RuntimeError</code></b>: if the Supervisor was created without a <code>logdir</code>.</li> </ul> <h3 id="wait_for_stop"><code>wait_for_stop()</code></h3> <p>Block waiting for the coordinator to stop.</p> <h2 id="class_members">Class Members</h2> <h3 id="USE_DEFAULT"><code>USE_DEFAULT</code></h3> <p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/python/training/supervisor.py" target="_blank"><code>tensorflow/python/training/supervisor.py</code></a>.</p>
<div class="_attribution">
  <p class="_attribution-p">
    Â© 2017 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/train/Supervisor" class="_attribution-link" target="_blank">https://www.tensorflow.org/api_docs/python/tf/train/Supervisor</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
