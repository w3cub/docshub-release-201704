
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Segmentation - Scikit-image - W3cubDocs</title>
  
  <meta name="description" content=" Active contour model. ">
  <meta name="keywords" content="module, segmentation, -, scikit-image, scikit_image">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_image/api/skimage.segmentation/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_image.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_image/" class="_nav-link" title="" style="margin-left:0;">scikit-image</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="module-segmentation">Module: segmentation</h1> <table class="longtable docutils" id="module-skimage.segmentation">   <tr class="row-odd">
<td>
<a class="reference internal" href="#skimage.segmentation.active_contour" title="skimage.segmentation.active_contour"><code>skimage.segmentation.active_contour</code></a>(image, snake)</td> <td>Active contour model.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#skimage.segmentation.clear_border" title="skimage.segmentation.clear_border"><code>skimage.segmentation.clear_border</code></a>(labels[, ...])</td> <td>Clear objects connected to the label image border.</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#skimage.segmentation.felzenszwalb" title="skimage.segmentation.felzenszwalb"><code>skimage.segmentation.felzenszwalb</code></a>(image[, ...])</td> <td>Computes Felsenszwalb’s efficient graph based image segmentation.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#skimage.segmentation.find_boundaries" title="skimage.segmentation.find_boundaries"><code>skimage.segmentation.find_boundaries</code></a>(label_img)</td> <td>Return bool array where boundaries between labeled regions are True.</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#skimage.segmentation.join_segmentations" title="skimage.segmentation.join_segmentations"><code>skimage.segmentation.join_segmentations</code></a>(s1, s2)</td> <td>Return the join of the two input segmentations.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#skimage.segmentation.mark_boundaries" title="skimage.segmentation.mark_boundaries"><code>skimage.segmentation.mark_boundaries</code></a>(image, ...)</td> <td>Return image with boundaries between labeled regions highlighted.</td> </tr> <tr class="row-odd">
<td><a class="reference internal" href="#skimage.segmentation.quickshift" title="skimage.segmentation.quickshift"><code>skimage.segmentation.quickshift</code></a></td> <td>Segments image using quickshift clustering in Color-(x,y) space.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#skimage.segmentation.random_walker" title="skimage.segmentation.random_walker"><code>skimage.segmentation.random_walker</code></a>(data, labels)</td> <td>Random walker algorithm for segmentation from markers.</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#skimage.segmentation.relabel_from_one" title="skimage.segmentation.relabel_from_one"><code>skimage.segmentation.relabel_from_one</code></a>(*args, ...)</td> <td>
<strong>Deprecated function</strong>. Use <code>relabel_sequential</code> instead.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#skimage.segmentation.relabel_sequential" title="skimage.segmentation.relabel_sequential"><code>skimage.segmentation.relabel_sequential</code></a>(...)</td> <td>Relabel arbitrary labels to {<code>offset</code>, ...</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#skimage.segmentation.slic" title="skimage.segmentation.slic"><code>skimage.segmentation.slic</code></a>(image[, ...])</td> <td>Segments image using k-means clustering in Color-(x,y,z) space.</td> </tr>  </table>  <h2 id="active-contour">active_contour</h2> <dl class="function"> <dt id="skimage.segmentation.active_contour">
<code>skimage.segmentation.active_contour(image, snake, alpha=0.01, beta=0.1, w_line=0, w_edge=1, gamma=0.01, bc='periodic', max_px_move=1.0, max_iterations=2500, convergence=0.1)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.12.2/skimage/segmentation/active_contour_model.py#L9" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Active contour model.</p> <p>Active contours by fitting snakes to features of images. Supports single and multichannel 2D images. Snakes can be periodic (for segmentation) or have fixed and/or free ends.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>image</strong> : (N, M) or (N, M, 3) ndarray</p>  <p>Input image.</p>  <p><strong>snake</strong> : (N, 2) ndarray</p>  <p>Initialisation coordinates of snake. For periodic snakes, it should not include duplicate endpoints.</p>  <p><strong>alpha</strong> : float, optional</p>  <p>Snake length shape parameter. Higher values makes snake contract faster.</p>  <p><strong>beta</strong> : float, optional</p>  <p>Snake smoothness shape parameter. Higher values makes snake smoother.</p>  <p><strong>w_line</strong> : float, optional</p>  <p>Controls attraction to brightness. Use negative values to attract to dark regions.</p>  <p><strong>w_edge</strong> : float, optional</p>  <p>Controls attraction to edges. Use negative values to repel snake from edges.</p>  <p><strong>gamma</strong> : float, optional</p>  <p>Explicit time stepping parameter.</p>  <p><strong>bc</strong> : {‘periodic’, ‘free’, ‘fixed’}, optional</p>  <p>Boundary conditions for worm. ‘periodic’ attaches the two ends of the snake, ‘fixed’ holds the end-points in place, and’free’ allows free movement of the ends. ‘fixed’ and ‘free’ can be combined by parsing ‘fixed-free’, ‘free-fixed’. Parsing ‘fixed-fixed’ or ‘free-free’ yields same behaviour as ‘fixed’ and ‘free’, respectively.</p>  <p><strong>max_px_move</strong> : float, optional</p>  <p>Maximum pixel distance to move per iteration.</p>  <p><strong>max_iterations</strong> : int, optional</p>  <p>Maximum iterations to optimize snake shape.</p>  <p><strong>convergence: float, optional</strong></p>  <p>Convergence criteria.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>snake</strong> : (N, 2) ndarray</p>  <p>Optimised snake, same shape as input parameter.</p>  </td> </tr>  </table> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="r347" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id1">[R347]</a></td>
<td>Kass, M.; Witkin, A.; Terzopoulos, D. “Snakes: Active contour models”. International Journal of Computer Vision 1 (4): 321 (1988).</td>
</tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from skimage.draw import circle_perimeter
&gt;&gt;&gt; from skimage.filters import gaussian_filter
</pre> <p>Create and smooth image:</p> <pre data-language="python">&gt;&gt;&gt; img = np.zeros((100, 100))
&gt;&gt;&gt; rr, cc = circle_perimeter(35, 45, 25)
&gt;&gt;&gt; img[rr, cc] = 1
&gt;&gt;&gt; img = gaussian_filter(img, 2)
</pre> <p>Initiliaze spline:</p> <pre data-language="python">&gt;&gt;&gt; s = np.linspace(0, 2*np.pi,100)
&gt;&gt;&gt; init = 50*np.array([np.cos(s), np.sin(s)]).T+50
</pre> <p>Fit spline to image:</p> <pre data-language="python">&gt;&gt;&gt; snake = active_contour(img, init, w_edge=0, w_line=1) 
&gt;&gt;&gt; dist = np.sqrt((45-snake[:, 0])**2 +(35-snake[:, 1])**2) 
&gt;&gt;&gt; int(np.mean(dist)) 
25
</pre> </dd>
</dl>   <h2 id="clear-border">clear_border</h2> <dl class="function"> <dt id="skimage.segmentation.clear_border">
<code>skimage.segmentation.clear_border(labels, buffer_size=0, bgval=0, in_place=False)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.12.2/skimage/segmentation/_clear_border.py#L5" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Clear objects connected to the label image border.</p> <p>The changes will be applied directly to the input.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>labels</strong> : (N, M) array of int</p>  <p>Label or binary image.</p>  <p><strong>buffer_size</strong> : int, optional</p>  <p>The width of the border examined. By default, only objects that touch the outside of the image are removed.</p>  <p><strong>bgval</strong> : float or int, optional</p>  <p>Cleared objects are set to this value.</p>  <p><strong>in_place</strong> : bool, optional</p>  <p>Whether or not to manipulate the labels array in-place.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>labels</strong> : (N, M) array</p>  <p>Cleared binary image.</p>  </td> </tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from skimage.segmentation import clear_border
&gt;&gt;&gt; labels = np.array([[0, 0, 0, 0, 0, 0, 0, 1, 0],
...                    [0, 0, 0, 0, 1, 0, 0, 0, 0],
...                    [1, 0, 0, 1, 0, 1, 0, 0, 0],
...                    [0, 0, 1, 1, 1, 1, 1, 0, 0],
...                    [0, 1, 1, 1, 1, 1, 1, 1, 0],
...                    [0, 0, 0, 0, 0, 0, 0, 0, 0]])
&gt;&gt;&gt; clear_border(labels)
array([[0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 1, 0, 0, 0, 0],
       [0, 0, 0, 1, 0, 1, 0, 0, 0],
       [0, 0, 1, 1, 1, 1, 1, 0, 0],
       [0, 1, 1, 1, 1, 1, 1, 1, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0]])
</pre> </dd>
</dl>   <h2 id="felzenszwalb">felzenszwalb</h2> <dl class="function"> <dt id="skimage.segmentation.felzenszwalb">
<code>skimage.segmentation.felzenszwalb(image, scale=1, sigma=0.8, min_size=20)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.12.2/skimage/segmentation/_felzenszwalb.py#L7" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Computes Felsenszwalb’s efficient graph based image segmentation.</p> <p>Produces an oversegmentation of a multichannel (i.e. RGB) image using a fast, minimum spanning tree based clustering on the image grid. The parameter <code>scale</code> sets an observation level. Higher scale means less and larger segments. <code>sigma</code> is the diameter of a Gaussian kernel, used for smoothing the image prior to segmentation.</p> <p>The number of produced segments as well as their size can only be controlled indirectly through <code>scale</code>. Segment size within an image can vary greatly depending on local contrast.</p> <p>For RGB images, the algorithm computes a separate segmentation for each channel and then combines these. The combined segmentation is the intersection of the separate segmentations on the color channels.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>image</strong> : (width, height, 3) or (width, height) ndarray</p>  <p>Input image.</p>  <p><strong>scale</strong> : float</p>  <p>Free parameter. Higher means larger clusters.</p>  <p><strong>sigma</strong> : float</p>  <p>Width of Gaussian kernel used in preprocessing.</p>  <p><strong>min_size</strong> : int</p>  <p>Minimum component size. Enforced using postprocessing.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>segment_mask</strong> : (width, height) ndarray</p>  <p>Integer mask indicating segment labels.</p>  </td> </tr>  </table> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="r348" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id2">[R348]</a></td>
<td>Efficient graph-based image segmentation, Felzenszwalb, P.F. and Huttenlocher, D.P. International Journal of Computer Vision, 2004</td>
</tr>  </table> </dd>
</dl>   <h2 id="find-boundaries">find_boundaries</h2> <dl class="function"> <dt id="skimage.segmentation.find_boundaries">
<code>skimage.segmentation.find_boundaries(label_img, connectivity=1, mode='thick', background=0)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.12.2/skimage/segmentation/boundaries.py#L49" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Return bool array where boundaries between labeled regions are True.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>label_img</strong> : array of int or bool</p>  <p>An array in which different regions are labeled with either different integers or boolean values.</p>  <p><strong>connectivity: int in {1, ..., `label_img.ndim`}, optional</strong></p>  <p>A pixel is considered a boundary pixel if any of its neighbors has a different label. <code>connectivity</code> controls which pixels are considered neighbors. A connectivity of 1 (default) means pixels sharing an edge (in 2D) or a face (in 3D) will be considered neighbors. A connectivity of <code>label_img.ndim</code> means pixels sharing a corner will be considered neighbors.</p>  <p><strong>mode: string in {‘thick’, ‘inner’, ‘outer’, ‘subpixel’}</strong></p>  <p>How to mark the boundaries:</p> <ul class="simple"> <li>thick: any pixel not completely surrounded by pixels of the same label (defined by <code>connectivity</code>) is marked as a boundary. This results in boundaries that are 2 pixels thick.</li> <li>inner: outline the pixels <em>just inside</em> of objects, leaving background pixels untouched.</li> <li>outer: outline pixels in the background around object boundaries. When two objects touch, their boundary is also marked.</li> <li>subpixel: return a doubled image, with pixels <em>between</em> the original pixels marked as boundary where appropriate.</li> </ul>  <p><strong>background: int, optional</strong></p>  <p>For modes ‘inner’ and ‘outer’, a definition of a background label is required. See <code>mode</code> for descriptions of these two.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>boundaries</strong> : array of bool, same shape as <code>label_img</code></p>  <p>A bool image where <code>True</code> represents a boundary pixel. For <code>mode</code> equal to ‘subpixel’, <code>boundaries.shape[i]</code> is equal to <code>2 * label_img.shape[i] - 1</code> for all <code>i</code> (a pixel is inserted in between all other pairs of pixels).</p>  </td> </tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; labels = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
...                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
...                    [0, 0, 0, 0, 0, 5, 5, 5, 0, 0],
...                    [0, 0, 1, 1, 1, 5, 5, 5, 0, 0],
...                    [0, 0, 1, 1, 1, 5, 5, 5, 0, 0],
...                    [0, 0, 1, 1, 1, 5, 5, 5, 0, 0],
...                    [0, 0, 0, 0, 0, 5, 5, 5, 0, 0],
...                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
...                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=np.uint8)
&gt;&gt;&gt; find_boundaries(labels, mode='thick').astype(np.uint8)
array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],
       [0, 0, 1, 1, 1, 1, 1, 1, 1, 0],
       [0, 1, 1, 1, 1, 1, 0, 1, 1, 0],
       [0, 1, 1, 0, 1, 1, 0, 1, 1, 0],
       [0, 1, 1, 1, 1, 1, 0, 1, 1, 0],
       [0, 0, 1, 1, 1, 1, 1, 1, 1, 0],
       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)
&gt;&gt;&gt; find_boundaries(labels, mode='inner').astype(np.uint8)
array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],
       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0],
       [0, 0, 1, 0, 1, 1, 0, 1, 0, 0],
       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0],
       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)
&gt;&gt;&gt; find_boundaries(labels, mode='outer').astype(np.uint8)
array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],
       [0, 0, 1, 1, 1, 1, 0, 0, 1, 0],
       [0, 1, 0, 0, 1, 1, 0, 0, 1, 0],
       [0, 1, 0, 0, 1, 1, 0, 0, 1, 0],
       [0, 1, 0, 0, 1, 1, 0, 0, 1, 0],
       [0, 0, 1, 1, 1, 1, 0, 0, 1, 0],
       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)
&gt;&gt;&gt; labels_small = labels[::2, ::3]
&gt;&gt;&gt; labels_small
array([[0, 0, 0, 0],
       [0, 0, 5, 0],
       [0, 1, 5, 0],
       [0, 0, 5, 0],
       [0, 0, 0, 0]], dtype=uint8)
&gt;&gt;&gt; find_boundaries(labels_small, mode='subpixel').astype(np.uint8)
array([[0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 1, 1, 1, 0],
       [0, 0, 0, 1, 0, 1, 0],
       [0, 1, 1, 1, 0, 1, 0],
       [0, 1, 0, 1, 0, 1, 0],
       [0, 1, 1, 1, 0, 1, 0],
       [0, 0, 0, 1, 0, 1, 0],
       [0, 0, 0, 1, 1, 1, 0],
       [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)
&gt;&gt;&gt; bool_image = np.array([[False, False, False, False, False],
...                        [False, False, False, False, False],
...                        [False, False,  True,  True,  True],
...                        [False, False,  True,  True,  True],
...                        [False, False,  True,  True,  True]], dtype=np.bool)
&gt;&gt;&gt; find_boundaries(bool_image)
array([[False, False, False, False, False],
       [False, False,  True,  True,  True],
       [False,  True,  True,  True,  True],
       [False,  True,  True, False, False],
       [False,  True,  True, False, False]], dtype=bool)
</pre> </dd>
</dl>   <h2 id="join-segmentations">join_segmentations</h2> <dl class="function"> <dt id="skimage.segmentation.join_segmentations">
<code>skimage.segmentation.join_segmentations(s1, s2)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.12.2/skimage/segmentation/_join.py#L5" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Return the join of the two input segmentations.</p> <p>The join J of S1 and S2 is defined as the segmentation in which two voxels are in the same segment if and only if they are in the same segment in <em>both</em> S1 and S2.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>s1, s2</strong> : numpy arrays</p>  <p>s1 and s2 are label fields of the same shape.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>j</strong> : numpy array</p>  <p>The join segmentation of s1 and s2.</p>  </td> </tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from skimage.segmentation import join_segmentations
&gt;&gt;&gt; s1 = np.array([[0, 0, 1, 1],
...                [0, 2, 1, 1],
...                [2, 2, 2, 1]])
&gt;&gt;&gt; s2 = np.array([[0, 1, 1, 0],
...                [0, 1, 1, 0],
...                [0, 1, 1, 1]])
&gt;&gt;&gt; join_segmentations(s1, s2)
array([[0, 1, 3, 2],
       [0, 5, 3, 2],
       [4, 5, 5, 3]])
</pre> </dd>
</dl>   <h2 id="mark-boundaries">mark_boundaries</h2> <dl class="function"> <dt id="skimage.segmentation.mark_boundaries">
<code>skimage.segmentation.mark_boundaries(image, label_img, color=(1, 1, 0), outline_color=None, mode='outer', background_label=0)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.12.2/skimage/segmentation/boundaries.py#L184" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Return image with boundaries between labeled regions highlighted.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>image</strong> : (M, N[, 3]) array</p>  <p>Grayscale or RGB image.</p>  <p><strong>label_img</strong> : (M, N) array of int</p>  <p>Label array where regions are marked by different integer values.</p>  <p><strong>color</strong> : length-3 sequence, optional</p>  <p>RGB color of boundaries in the output image.</p>  <p><strong>outline_color</strong> : length-3 sequence, optional</p>  <p>RGB color surrounding boundaries in the output image. If None, no outline is drawn.</p>  <p><strong>mode</strong> : string in {‘thick’, ‘inner’, ‘outer’, ‘subpixel’}, optional</p>  <p>The mode for finding boundaries.</p>  <p><strong>background_label</strong> : int, optional</p>  <p>Which label to consider background (this is only useful for modes <code>inner</code> and <code>outer</code>).</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>marked</strong> : (M, N, 3) array of float</p>  <p>An image in which the boundaries between labels are superimposed on the original image.</p>  </td> </tr>  </table> <div class="admonition seealso"> <p class="first admonition-title">See also</p> <p class="last"><a class="reference internal" href="#skimage.segmentation.find_boundaries" title="skimage.segmentation.find_boundaries"><code>find_boundaries</code></a></p> </div> </dd>
</dl>   <h2 id="quickshift">quickshift</h2> <dl class="function"> <dt id="skimage.segmentation.quickshift">
<code>skimage.segmentation.quickshift()</code> </dt> <dd>
<p>Segments image using quickshift clustering in Color-(x,y) space.</p> <p>Produces an oversegmentation of the image using the quickshift mode-seeking algorithm.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>image</strong> : (width, height, channels) ndarray</p>  <p>Input image.</p>  <p><strong>ratio</strong> : float, optional, between 0 and 1 (default 1).</p>  <p>Balances color-space proximity and image-space proximity. Higher values give more weight to color-space.</p>  <p><strong>kernel_size</strong> : float, optional (default 5)</p>  <p>Width of Gaussian kernel used in smoothing the sample density. Higher means fewer clusters.</p>  <p><strong>max_dist</strong> : float, optional (default 10)</p>  <p>Cut-off point for data distances. Higher means fewer clusters.</p>  <p><strong>return_tree</strong> : bool, optional (default False)</p>  <p>Whether to return the full segmentation hierarchy tree and distances.</p>  <p><strong>sigma</strong> : float, optional (default 0)</p>  <p>Width for Gaussian smoothing as preprocessing. Zero means no smoothing.</p>  <p><strong>convert2lab</strong> : bool, optional (default True)</p>  <p>Whether the input should be converted to Lab colorspace prior to segmentation. For this purpose, the input is assumed to be RGB.</p>  <p><strong>random_seed</strong> : None (default) or int, optional</p>  <p>Random seed used for breaking ties.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>segment_mask</strong> : (width, height) ndarray</p>  <p>Integer mask indicating segment labels.</p>  </td> </tr>  </table> <h4 class="rubric">Notes</h4> <p>The authors advocate to convert the image to Lab color space prior to segmentation, though this is not strictly necessary. For this to work, the image must be given in RGB format.</p> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="r349" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id3">[R349]</a></td>
<td>Quick shift and kernel methods for mode seeking, Vedaldi, A. and Soatto, S. European Conference on Computer Vision, 2008</td>
</tr>  </table> </dd>
</dl>   <h2 id="random-walker">random_walker</h2> <dl class="function"> <dt id="skimage.segmentation.random_walker">
<code>skimage.segmentation.random_walker(data, labels, beta=130, mode='bf', tol=0.001, copy=True, multichannel=False, return_full_prob=False, spacing=None)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.12.2/skimage/segmentation/random_walker_segmentation.py#L194" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Random walker algorithm for segmentation from markers.</p> <p>Random walker algorithm is implemented for gray-level or multichannel images.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>data</strong> : array_like</p>  <p>Image to be segmented in phases. Gray-level <code>data</code> can be two- or three-dimensional; multichannel data can be three- or four- dimensional (multichannel=True) with the highest dimension denoting channels. Data spacing is assumed isotropic unless the <code>spacing</code> keyword argument is used.</p>  <p><strong>labels</strong> : array of ints, of same shape as <code>data</code> without channels dimension</p>  <p>Array of seed markers labeled with different positive integers for different phases. Zero-labeled pixels are unlabeled pixels. Negative labels correspond to inactive pixels that are not taken into account (they are removed from the graph). If labels are not consecutive integers, the labels array will be transformed so that labels are consecutive. In the multichannel case, <code>labels</code> should have the same shape as a single channel of <code>data</code>, i.e. without the final dimension denoting channels.</p>  <p><strong>beta</strong> : float</p>  <p>Penalization coefficient for the random walker motion (the greater <code>beta</code>, the more difficult the diffusion).</p>  <p><strong>mode</strong> : string, available options {‘cg_mg’, ‘cg’, ‘bf’}</p>  <p>Mode for solving the linear system in the random walker algorithm. If no preference given, automatically attempt to use the fastest option available (‘cg_mg’ from pyamg &gt;&gt; ‘cg’ with UMFPACK &gt; ‘bf’).</p> <ul class="simple"> <li>‘bf’ (brute force): an LU factorization of the Laplacian is computed. This is fast for small images (&lt;1024x1024), but very slow and memory-intensive for large images (e.g., 3-D volumes).</li> <li>‘cg’ (conjugate gradient): the linear system is solved iteratively using the Conjugate Gradient method from scipy.sparse.linalg. This is less memory-consuming than the brute force method for large images, but it is quite slow.</li> <li>‘cg_mg’ (conjugate gradient with multigrid preconditioner): a preconditioner is computed using a multigrid solver, then the solution is computed with the Conjugate Gradient method. This mode requires that the pyamg module (<a class="reference external" href="http://pyamg.org/" target="_blank">http://pyamg.org/</a>) is installed. For images of size &gt; 512x512, this is the recommended (fastest) mode.</li> </ul>  <p><strong>tol</strong> : float</p>  <p>tolerance to achieve when solving the linear system, in cg’ and ‘cg_mg’ modes.</p>  <p><strong>copy</strong> : bool</p>  <p>If copy is False, the <code>labels</code> array will be overwritten with the result of the segmentation. Use copy=False if you want to save on memory.</p>  <p><strong>multichannel</strong> : bool, default False</p>  <p>If True, input data is parsed as multichannel data (see ‘data’ above for proper input format in this case)</p>  <p><strong>return_full_prob</strong> : bool, default False</p>  <p>If True, the probability that a pixel belongs to each of the labels will be returned, instead of only the most likely label.</p>  <p><strong>spacing</strong> : iterable of floats</p>  <p>Spacing between voxels in each spatial dimension. If <code>None</code>, then the spacing between pixels/voxels in each dimension is assumed 1.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>output</strong> : ndarray</p>  <ul class="simple"> <li>If <code>return_full_prob</code> is False, array of ints of same shape as <code>data</code>, in which each pixel has been labeled according to the marker that reached the pixel first by anisotropic diffusion.</li> <li>If <code>return_full_prob</code> is True, array of floats of shape <code>(nlabels, data.shape)</code>. <code>output[label_nb, i, j]</code> is the probability that label <code>label_nb</code> reaches the pixel <code>(i, j)</code> first.</li> </ul>  </td> </tr>  </table> <div class="admonition seealso"> <p class="first admonition-title">See also</p> <dl class="last docutils"> <dt>
 <a class="reference internal" href="../skimage.morphology/#skimage.morphology.watershed" title="skimage.morphology.watershed"><code>skimage.morphology.watershed</code></a>
</dt> <dd>watershed segmentation A segmentation algorithm based on mathematical morphology and “flooding” of regions from markers.</dd> </dl> </div> <h4 class="rubric">Notes</h4> <p>Multichannel inputs are scaled with all channel data combined. Ensure all channels are separately normalized prior to running this algorithm.</p> <p>The <code>spacing</code> argument is specifically for anisotropic datasets, where data points are spaced differently in one or more spatial dimensions. Anisotropic data is commonly encountered in medical imaging.</p> <p>The algorithm was first proposed in <em>Random walks for image segmentation</em>, Leo Grady, IEEE Trans Pattern Anal Mach Intell. 2006 Nov;28(11):1768-83.</p> <p>The algorithm solves the diffusion equation at infinite times for sources placed on markers of each phase in turn. A pixel is labeled with the phase that has the greatest probability to diffuse first to the pixel.</p> <p>The diffusion equation is solved by minimizing x.T L x for each phase, where L is the Laplacian of the weighted graph of the image, and x is the probability that a marker of the given phase arrives first at a pixel by diffusion (x=1 on markers of the phase, x=0 on the other markers, and the other coefficients are looked for). Each pixel is attributed the label for which it has a maximal value of x. The Laplacian L of the image is defined as:</p>  <ul class="simple"> <li>L_ii = d_i, the number of neighbors of pixel i (the degree of i)</li> <li>L_ij = -w_ij if i and j are adjacent pixels</li> </ul>  <p>The weight w_ij is a decreasing function of the norm of the local gradient. This ensures that diffusion is easier between pixels of similar values.</p> <p>When the Laplacian is decomposed into blocks of marked and unmarked pixels:</p> <pre data-language="python">L = M B.T
    B A
</pre> <p>with first indices corresponding to marked pixels, and then to unmarked pixels, minimizing x.T L x for one phase amount to solving:</p> <pre data-language="python">A x = - B x_m
</pre> <p>where x_m = 1 on markers of the given phase, and 0 on other markers. This linear system is solved in the algorithm using a direct method for small images, and an iterative method for larger images.</p> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; np.random.seed(0)
&gt;&gt;&gt; a = np.zeros((10, 10)) + 0.2 * np.random.rand(10, 10)
&gt;&gt;&gt; a[5:8, 5:8] += 1
&gt;&gt;&gt; b = np.zeros_like(a)
&gt;&gt;&gt; b[3, 3] = 1  # Marker for first phase
&gt;&gt;&gt; b[6, 6] = 2  # Marker for second phase
&gt;&gt;&gt; random_walker(a, b)
array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],
       [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],
       [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)
</pre> </dd>
</dl>   <h2 id="relabel-from-one">relabel_from_one</h2> <dl class="function"> <dt id="skimage.segmentation.relabel_from_one">
<code>skimage.segmentation.relabel_from_one(*args, **kwargs)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.12.2/skimage/_shared/utils.py#L50" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p><strong>Deprecated function</strong>. Use <code>relabel_sequential</code> instead.</p> <p>Convert labels in an arbitrary label field to {1, ... number_of_labels}.</p> <p>This function is deprecated, see <code>relabel_sequential</code> for more.</p> </dd>
</dl>   <h2 id="relabel-sequential">relabel_sequential</h2> <dl class="function"> <dt id="skimage.segmentation.relabel_sequential">
<code>skimage.segmentation.relabel_sequential(label_field, offset=1)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.12.2/skimage/segmentation/_join.py#L55" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Relabel arbitrary labels to {<code>offset</code>, ... <code>offset</code> + number_of_labels}.</p> <p>This function also returns the forward map (mapping the original labels to the reduced labels) and the inverse map (mapping the reduced labels back to the original ones).</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>label_field</strong> : numpy array of int, arbitrary shape</p>  <p>An array of labels.</p>  <p><strong>offset</strong> : int, optional</p>  <p>The return labels will start at <code>offset</code>, which should be strictly positive.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>relabeled</strong> : numpy array of int, same shape as <code>label_field</code></p>  <p>The input label field with labels mapped to {offset, ..., number_of_labels + offset - 1}.</p>  <p><strong>forward_map</strong> : numpy array of int, shape <code>(label_field.max() + 1,)</code></p>  <p>The map from the original label space to the returned label space. Can be used to re-apply the same mapping. See examples for usage.</p>  <p><strong>inverse_map</strong> : 1D numpy array of int, of length offset + number of labels</p>  <p>The map from the new label space to the original space. This can be used to reconstruct the original label field from the relabeled one.</p>  </td> </tr>  </table> <h4 class="rubric">Notes</h4> <p>The label 0 is assumed to denote the background and is never remapped.</p> <p>The forward map can be extremely big for some inputs, since its length is given by the maximum of the label field. However, in most situations, <code>label_field.max()</code> is much smaller than <code>label_field.size</code>, and in these cases the forward map is guaranteed to be smaller than either the input or output images.</p> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from skimage.segmentation import relabel_sequential
&gt;&gt;&gt; label_field = np.array([1, 1, 5, 5, 8, 99, 42])
&gt;&gt;&gt; relab, fw, inv = relabel_sequential(label_field)
&gt;&gt;&gt; relab
array([1, 1, 2, 2, 3, 5, 4])
&gt;&gt;&gt; fw
array([0, 1, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 5])
&gt;&gt;&gt; inv
array([ 0,  1,  5,  8, 42, 99])
&gt;&gt;&gt; (fw[label_field] == relab).all()
True
&gt;&gt;&gt; (inv[relab] == label_field).all()
True
&gt;&gt;&gt; relab, fw, inv = relabel_sequential(label_field, offset=5)
&gt;&gt;&gt; relab
array([5, 5, 6, 6, 7, 9, 8])
</pre> </dd>
</dl>   <h2 id="slic">slic</h2> <dl class="function"> <dt id="skimage.segmentation.slic">
<code>skimage.segmentation.slic(image, n_segments=100, compactness=10.0, max_iter=10, sigma=0, spacing=None, multichannel=True, convert2lab=None, enforce_connectivity=True, min_size_factor=0.5, max_size_factor=3, slic_zero=False)</code> <a class="reference external" href="http://github.com/scikit-image/scikit-image/blob/v0.12.2/skimage/segmentation/slic_superpixels.py#L14" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Segments image using k-means clustering in Color-(x,y,z) space.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>image</strong> : 2D, 3D or 4D ndarray</p>  <p>Input image, which can be 2D or 3D, and grayscale or multichannel (see <code>multichannel</code> parameter).</p>  <p><strong>n_segments</strong> : int, optional</p>  <p>The (approximate) number of labels in the segmented output image.</p>  <p><strong>compactness</strong> : float, optional</p>  <p>Balances color proximity and space proximity. Higher values give more weight to space proximity, making superpixel shapes more square/cubic. In SLICO mode, this is the initial compactness. This parameter depends strongly on image contrast and on the shapes of objects in the image. We recommend exploring possible values on a log scale, e.g., 0.01, 0.1, 1, 10, 100, before refining around a chosen value.</p>  <p><strong>max_iter</strong> : int, optional</p>  <p>Maximum number of iterations of k-means.</p>  <p><strong>sigma</strong> : float or (3,) array-like of floats, optional</p>  <p>Width of Gaussian smoothing kernel for pre-processing for each dimension of the image. The same sigma is applied to each dimension in case of a scalar value. Zero means no smoothing. Note, that <code>sigma</code> is automatically scaled if it is scalar and a manual voxel spacing is provided (see Notes section).</p>  <p><strong>spacing</strong> : (3,) array-like of floats, optional</p>  <p>The voxel spacing along each image dimension. By default, <code>slic</code> assumes uniform spacing (same voxel resolution along z, y and x). This parameter controls the weights of the distances along z, y, and x during k-means clustering.</p>  <p><strong>multichannel</strong> : bool, optional</p>  <p>Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.</p>  <p><strong>convert2lab</strong> : bool, optional</p>  <p>Whether the input should be converted to Lab colorspace prior to segmentation. The input image <em>must</em> be RGB. Highly recommended. This option defaults to <code>True</code> when <code>multichannel=True</code> <em>and</em> <code>image.shape[-1] == 3</code>.</p>  <p><strong>enforce_connectivity: bool, optional</strong></p>  <p>Whether the generated segments are connected or not</p>  <p><strong>min_size_factor: float, optional</strong></p>  <p>Proportion of the minimum segment size to be removed with respect to the supposed segment size <code>`depth*width*height/n_segments`</code></p>  <p><strong>max_size_factor: float, optional</strong></p>  <p>Proportion of the maximum connected segment size. A value of 3 works in most of the cases.</p>  <p><strong>slic_zero: bool, optional</strong></p>  <p>Run SLIC-zero, the zero-parameter mode of SLIC. <a class="reference internal" href="#r351" id="id4">[R351]</a></p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>labels</strong> : 2D or 3D array</p>  <p>Integer mask indicating segment labels.</p>  </td> </tr> <tr class="field-odd field">
<th class="field-name">Raises:</th>
<td class="field-body">
<p class="first"><strong>ValueError</strong></p>  <p>If <code>convert2lab</code> is set to <code>True</code> but the last array dimension is not of length 3.</p>  </td> </tr>  </table> <h4 class="rubric">Notes</h4> <ul class="simple"> <li>If <code>sigma &gt; 0</code>, the image is smoothed using a Gaussian kernel prior to segmentation.</li> <li>If <code>sigma</code> is scalar and <code>spacing</code> is provided, the kernel width is divided along each dimension by the spacing. For example, if <code>sigma=1</code> and <code>spacing=[5, 1, 1]</code>, the effective <code>sigma</code> is <code>[0.2, 1, 1]</code>. This ensures sensible smoothing for anisotropic images.</li> <li>The image is rescaled to be in [0, 1] prior to processing.</li> <li>Images of shape (M, N, 3) are interpreted as 2D RGB images by default. To interpret them as 3D with the last dimension having length 3, use <code>multichannel=False</code>.</li> </ul> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="r350" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id5">[R350]</a></td>
<td>Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk, SLIC Superpixels Compared to State-of-the-art Superpixel Methods, TPAMI, May 2012.</td>
</tr>  </table> <table class="docutils citation" frame="void" id="r351" rules="none">   <tr>
<td class="label">[R351]</td>
<td>
<em>(<a class="fn-backref" href="#id4">1</a>, <a class="fn-backref" href="#id6">2</a>)</em> <a class="reference external" href="http://ivrg.epfl.ch/research/superpixels#SLICO" target="_blank">http://ivrg.epfl.ch/research/superpixels#SLICO</a>
</td>
</tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from skimage.segmentation import slic
&gt;&gt;&gt; from skimage.data import astronaut
&gt;&gt;&gt; img = astronaut()
&gt;&gt;&gt; segments = slic(img, n_segments=100, compactness=10)
</pre> <p>Increasing the compactness parameter yields more square regions:</p> <pre data-language="python">&gt;&gt;&gt; segments = slic(img, n_segments=100, compactness=20)
</pre> </dd>
</dl>
<div class="_attribution">
  <p class="_attribution-p">
    © 2011 the scikit-image team<br>Licensed under the BSD 3-clause License.<br>
    <a href="http://scikit-image.org/docs/0.12.x/api/skimage.segmentation.html" class="_attribution-link" target="_blank">http://scikit-image.org/docs/0.12.x/api/skimage.segmentation.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
